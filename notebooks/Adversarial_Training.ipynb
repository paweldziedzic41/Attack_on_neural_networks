{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 20318,
     "status": "ok",
     "timestamp": 1733859420080,
     "user": {
      "displayName": "Escooobar",
      "userId": "12228491276356832373"
     },
     "user_tz": -60
    },
    "id": "8sctoRDCg9qU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytaj\n",
    "# - model.h5\n",
    "# - train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3550,
     "status": "ok",
     "timestamp": 1733859547684,
     "user": {
      "displayName": "Escooobar",
      "userId": "12228491276356832373"
     },
     "user_tz": -60
    },
    "id": "dcKOaAk6gf55",
    "outputId": "c25c946c-4ded-4288-e878-85e1e55934b9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1733859547684,
     "user": {
      "displayName": "Escooobar",
      "userId": "12228491276356832373"
     },
     "user_tz": -60
    },
    "id": "bRrrrp1hhClb"
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "            featurewise_center=False,  \n",
    "            samplewise_center=False,  \n",
    "            featurewise_std_normalization=False,  \n",
    "            samplewise_std_normalization=False,  \n",
    "            zca_whitening=False, \n",
    "            rotation_range=10,  \n",
    "            zoom_range = 0.1, \n",
    "            width_shift_range=0.1, \n",
    "            height_shift_range=0.1,  \n",
    "            horizontal_flip=False,  \n",
    "            vertical_flip=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1733859547684,
     "user": {
      "displayName": "Escooobar",
      "userId": "12228491276356832373"
     },
     "user_tz": -60
    },
    "id": "_UY0x1D2mNCT"
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', strides=1, padding='same', data_format='channels_last',\n",
    "                     input_shape=(28, 28, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', strides=1, padding='same', data_format='channels_last'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=2, padding='valid'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', strides=1, padding='same', data_format='channels_last'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), strides=1, padding='same', activation='relu', data_format='channels_last'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='valid', strides=2))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "\n",
    "def adversarial_training(x_train, y_train, model, x_test, y_test, batch_size, epochs, split, attack_type, c):\n",
    "\n",
    "    print(f\"\\n### Rozpoczynam trenowanie dla splitu {split * 100}% ###\\n\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_adversarial_train = int(split * len(x_train))\n",
    "    print(f\"Generowanie {num_adversarial_train} przykładów adwersarialnych...\")\n",
    "\n",
    "    adversarial_batch_size_train = 63  \n",
    "    adversarial_images_train = []\n",
    "\n",
    "    for i in range(0, num_adversarial_train, adversarial_batch_size_train):\n",
    "      batch_images = x_train[i:i + adversarial_batch_size_train]\n",
    "      batch_labels = y_train[i:i + adversarial_batch_size_train]\n",
    "      print(f\"Od {i} do {i + adversarial_batch_size_train} z {num_adversarial_train}\")\n",
    "      if attack_type == 'PGD':\n",
    "        adversarial_images_train.extend(\n",
    "            pgd_attack(\n",
    "                model=model,\n",
    "                images=batch_images,\n",
    "                labels=batch_labels,\n",
    "                epsilon=0.1,\n",
    "                c=c,\n",
    "                num_iterations=50\n",
    "            )\n",
    "        )\n",
    "      elif attack_type == 'CW':\n",
    "        adversarial_images_train.extend(cw_attack(\n",
    "              model=model,\n",
    "              images=batch_images,\n",
    "              labels=batch_labels,\n",
    "              norm=\"L2\",\n",
    "              c=c,\n",
    "              kappa=0,\n",
    "              max_iter=50,\n",
    "              learning_rate=0.01\n",
    "          ))\n",
    "\n",
    "    print(f\"Przykłady adwersarialne treningowe wygenerowane w czasie: {time.time() - start_time:.2f} s\")\n",
    "\n",
    "\n",
    "    num_adversarial_test = int(split * len(x_test))\n",
    "    print(f\"Generowanie {num_adversarial_train} przykładów adwersarialnych...\")\n",
    "\n",
    "    adversarial_batch_size_test = 42  # or even smaller, adjust as needed\n",
    "    adversarial_images_test = []\n",
    "\n",
    "    for i in range(0, num_adversarial_test, adversarial_batch_size_test):\n",
    "      print(f\"Od {i} do {i + adversarial_batch_size_test} z {num_adversarial_test}\")\n",
    "      batch_images = x_train[i:i + adversarial_batch_size_test]\n",
    "      batch_labels = y_train[i:i + adversarial_batch_size_test]\n",
    "      if attack_type == 'PGD':\n",
    "        adversarial_images_test.extend(\n",
    "            pgd_attack(\n",
    "                model=model,\n",
    "                images=batch_images,\n",
    "                labels=batch_labels,\n",
    "                norm_type='L2',\n",
    "                epsilon=0.1,\n",
    "                c=c,\n",
    "                num_iterations=50\n",
    "            )\n",
    "        )\n",
    "      elif attack_type == 'CW':\n",
    "        adversarial_images_test.extend(cw_attack(\n",
    "              model=model,\n",
    "              images=batch_images,\n",
    "              labels=batch_labels,\n",
    "              norm=\"L2\",\n",
    "              c=c,\n",
    "              kappa=0,\n",
    "              max_iter=50,\n",
    "              learning_rate=0.01\n",
    "          ))\n",
    "\n",
    "    adversarial_images_train = np.array(adversarial_images_train)\n",
    "    adversarial_labels_train = y_train[:num_adversarial_train]\n",
    "\n",
    "    adversarial_images_test = np.array(adversarial_images_test)\n",
    "    adversarial_labels_test = y_test[:num_adversarial_test]\n",
    "\n",
    "    # Combine Original and Adversarial Data\n",
    "    print(\"Łączenie danych oryginalnych i adwersarialnych...\")\n",
    "    x_train_combined = np.concatenate([x_train, adversarial_images_train])\n",
    "    y_train_combined = np.concatenate([y_train, adversarial_labels_train])\n",
    "\n",
    "    x_test_combined = np.concatenate([x_test, adversarial_images_test])\n",
    "    y_test_combined = np.concatenate([y_test, adversarial_labels_test])\n",
    "\n",
    "    print(f\"x_train_combined shape: {x_train_combined.shape}\")\n",
    "    print(f\"y_train_combined shape: {y_train_combined.shape}\")\n",
    "\n",
    "    print(f\"x_test_combined shape: {x_test_combined.shape}\")\n",
    "    print(f\"y_test_combined shape: {y_test_combined.shape}\")\n",
    "\n",
    "    # Create and Compile a New Model\n",
    "    new_model = create_model()\n",
    "    optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "    new_model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    # Learning Rate Scheduler\n",
    "    reduce_lr = LearningRateScheduler(lambda x: 1e-3 * 0.9 ** x)\n",
    "\n",
    "    # Early Stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        min_delta=0.001,\n",
    "        patience=20,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    print(\"Rozpoczęcie trenowania modelu...\")\n",
    "    # Trening modelu pomocniczego\n",
    "    history = new_model.fit(\n",
    "        datagen.flow(x_train_combined, y_train_combined, batch_size=64),\n",
    "        epochs=50,\n",
    "        validation_data=(x_test_combined, y_test_combined),\n",
    "        verbose=1,\n",
    "        steps_per_epoch=x_train_combined.shape[0] // 64,\n",
    "        callbacks=[reduce_lr, early_stopping]\n",
    "    )\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Model wytrenowany dla splitu {split * 100}% w czasie: {elapsed_time:.2f} s\\n\")\n",
    "\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 251,
     "status": "ok",
     "timestamp": 1733859639569,
     "user": {
      "displayName": "Escooobar",
      "userId": "12228491276356832373"
     },
     "user_tz": -60
    },
    "id": "woiePn2itTXY"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import statistics as stat\n",
    "\n",
    "def cw_attack(model, images, labels, norm='L2', c=1e-4, kappa=0, max_iter=1000, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Atak CW z obsługą norm L2, Linf.\n",
    "    Argumenty:\n",
    "        model: Model Keras, na który przeprowadzany jest atak.\n",
    "        images: Obrazy wejściowe, przeskalowane do zakresu [0, 1].\n",
    "        labels: Prawdziwe etykiety dla ataku nietargetowanego lub etykiety docelowe dla ataku targetowanego (zakodowane w formie one-hot).\n",
    "        norm: Typ normy dla perturbacji ('L2', 'Linf').\n",
    "        c: Parametr regularyzacyjny kontrolujący rozmiar perturbacji.\n",
    "        kappa: Margines pewności ataku.\n",
    "        max_iter: Maksymalna liczba kroków optymalizacji.\n",
    "        learning_rate: Współczynnik uczenia dla optymalizatora.\n",
    "    Zwraca:\n",
    "        Zmienione obrazy zawierające atak adversarialny.\n",
    "    \"\"\"\n",
    "\n",
    "    # Konwersja obrazów i etykiet na tensory\n",
    "    original_images = tf.convert_to_tensor(images, dtype=tf.float32)\n",
    "    true_labels = tf.convert_to_tensor(labels, dtype=tf.float32)\n",
    "\n",
    "    # Inicjalizacja perturbacji w przestrzeni tanh\n",
    "    w = tf.Variable(tf.zeros_like(original_images), trainable=True, dtype=tf.float32)\n",
    "\n",
    "    # Optymalizator\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # Definicja funkcji f\n",
    "    def f(x):\n",
    "        logits = model(x)\n",
    "        one_hot_labels = true_labels\n",
    "        true_logits = tf.reduce_sum(one_hot_labels * logits, axis=1)\n",
    "        other_logits = tf.reduce_max((1 - one_hot_labels) * logits - one_hot_labels * 1e4, axis=1)\n",
    "        return tf.maximum(true_logits - other_logits, -kappa)\n",
    "\n",
    "    # Definicja funkcji normy perturbacji\n",
    "    def perturbation_norm(perturbed_images, original_images, norm):\n",
    "        if norm == 'L2':\n",
    "            return tf.reduce_sum(tf.square(perturbed_images - original_images), axis=[1, 2, 3])\n",
    "        elif norm == 'Linf':\n",
    "            return tf.reduce_max(tf.abs(perturbed_images - original_images), axis=[1, 2, 3])\n",
    "        else:\n",
    "            raise ValueError(f\"Nieobsługiwana norma: {norm}\")\n",
    "\n",
    "    # Wykonanie optymalizacji\n",
    "    prev_loss = 1e10\n",
    "    for step in range(max_iter):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Mapowanie w z powrotem do przestrzeni obrazów przy użyciu funkcji tanh\n",
    "            perturbed_images = 0.5 * (tf.tanh(w) + 1)\n",
    "\n",
    "            # Obliczanie składowych funkcji straty\n",
    "            norm_loss = perturbation_norm(perturbed_images, original_images, norm)\n",
    "            f_loss = c * f(perturbed_images)\n",
    "            total_loss = tf.reduce_sum(norm_loss + f_loss)\n",
    "\n",
    "        # Obliczanie gradientów i aktualizacja wartości\n",
    "        gradients = tape.gradient(total_loss, [w])\n",
    "        optimizer.apply_gradients(zip(gradients, [w]))\n",
    "\n",
    "        # Sprawdzanie warunku wcześniejszego zatrzymania\n",
    "        if step % (max_iter // 10) == 0:\n",
    "            #print(f\"C:{c} Krok {step}/{max_iter}, Strata: {total_loss.numpy()}, norm_loss: {stat.mean(norm_loss.numpy())}, f_loss: {stat.mean(f_loss.numpy())}\")\n",
    "            if total_loss.numpy() > prev_loss:\n",
    "                print(\"Atak zatrzymany z powodu konwergencji.\")\n",
    "                break\n",
    "            prev_loss = total_loss.numpy()\n",
    "\n",
    "    # Mapowanie końcowego w z powrotem do przestrzeni obrazów\n",
    "    attack_images = 0.5 * (tf.tanh(w) + 1)\n",
    "    return attack_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 377,
     "status": "ok",
     "timestamp": 1733859642206,
     "user": {
      "displayName": "Escooobar",
      "userId": "12228491276356832373"
     },
     "user_tz": -60
    },
    "id": "DEdojvUEhEZR"
   },
   "outputs": [],
   "source": [
    "def pgd_attack(model, images, labels, epsilon, c, num_iterations, norm_type='L2', threshold=None):\n",
    "    \"\"\"\n",
    "    Przeprowadza atak PGD z kontrolą normy perturbacji i możliwością wcześniejszego zakończenia.\n",
    "\n",
    "    Parametry:\n",
    "    - model: Model Keras lub TensorFlow\n",
    "    - images: Obrazy wejściowe (batch)\n",
    "    - labels: Etykiety one-hot dla obrazów\n",
    "    - epsilon: Maksymalna wielkość perturbacji\n",
    "    - alpha: Krok każdej iteracji\n",
    "    - num_iterations: Maksymalna liczba iteracji\n",
    "    - norm_type: Typ normy ( 'l2', 'linf')\n",
    "    - threshold: Próg zmiany perturbacji dla wcześniejszego zakończenia (opcjonalny)\n",
    "\n",
    "    Zwraca:\n",
    "    - Adwersarialne obrazy\n",
    "    \"\"\"\n",
    "    # Inicjalizujemy perturbacje jako kopię obrazów wejściowych\n",
    "    perturbed_images = tf.identity(images)\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(perturbed_images)\n",
    "            predictions = model(perturbed_images)\n",
    "            loss = tf.keras.losses.categorical_crossentropy(labels, predictions)\n",
    "\n",
    "        # Obliczamy gradient\n",
    "        gradient = tape.gradient(loss, perturbed_images)\n",
    "\n",
    "        # Generowanie perturbacji na podstawie wybranej normy\n",
    "        if norm_type == 'Linf':\n",
    "            perturbation = c * tf.sign(gradient)\n",
    "        elif norm_type == 'L2':\n",
    "            # Ręczne obliczenie normy L2\n",
    "            norm = tf.sqrt(tf.reduce_sum(tf.square(gradient), axis=(1, 2, 3), keepdims=True))\n",
    "            perturbation = c * gradient / (norm + 1e-10)\n",
    "        else:\n",
    "            raise ValueError(\"Nieobsługiwana norma. Użyj 'l2' lub 'linf'.\")\n",
    "\n",
    "        # Aktualizacja obrazów adwersarialnych\n",
    "        new_perturbed_images = perturbed_images + perturbation\n",
    "        new_perturbed_images = tf.clip_by_value(new_perturbed_images, images - epsilon, images + epsilon)  # projekcja\n",
    "        new_perturbed_images = tf.clip_by_value(new_perturbed_images, 0, 1)  # ograniczenie do zakresu [0,1]\n",
    "\n",
    "        # Sprawdzenie progu zmiany perturbacji\n",
    "        if threshold is not None:\n",
    "            delta = tf.reduce_max(tf.abs(new_perturbed_images - perturbed_images))\n",
    "            if delta < threshold:\n",
    "                print(f\"Zatrzymanie po {i + 1} iteracjach (zmiana < {threshold})\")\n",
    "                break\n",
    "\n",
    "        perturbed_images = new_perturbed_images\n",
    "\n",
    "    return perturbed_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 4482,
     "status": "ok",
     "timestamp": 1733859649804,
     "user": {
      "displayName": "Escooobar",
      "userId": "12228491276356832373"
     },
     "user_tz": -60
    },
    "id": "jF4q3-6chGYo"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "\n",
    "# X is the pixels and Y is the image labels\n",
    "X = data.iloc[:,1:]\n",
    "Y = data.iloc[:,0]\n",
    "#first param in reshape is number of examples. We can pass -1 here as we want numpy to figure that out by itself\n",
    "\n",
    "#reshape(examples, height, width, channels)\n",
    "X_reshaped = X.values.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 363,
     "status": "ok",
     "timestamp": 1733859650165,
     "user": {
      "displayName": "Escooobar",
      "userId": "12228491276356832373"
     },
     "user_tz": -60
    },
    "id": "7fJVxej32rjc"
   },
   "outputs": [],
   "source": [
    "#splitting dataframe using train_test_split\n",
    "x_train , x_test , y_train , y_test = train_test_split(X_reshaped, Y, test_size=0.1)\n",
    "\n",
    "#convert values to float as result will be a float. If not done vals are set to zero\n",
    "x_train = x_train.astype(\"float32\")/255\n",
    "x_test = x_test.astype(\"float32\")/255\n",
    "\n",
    "#fitting the ImageDataGenerator\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1733859650165,
     "user": {
      "displayName": "Escooobar",
      "userId": "12228491276356832373"
     },
     "user_tz": -60
    },
    "id": "6qR1nmnZ2t_J"
   },
   "outputs": [],
   "source": [
    "#notice num_classes is set to 10 as we have 10 different labels\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1733859650165,
     "user": {
      "displayName": "Escooobar",
      "userId": "12228491276356832373"
     },
     "user_tz": -60
    },
    "id": "ThLvoy8EICMe"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "splits = [0.2, 0.3, 0.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 2313060,
     "status": "ok",
     "timestamp": 1733864880827,
     "user": {
      "displayName": "Escooobar",
      "userId": "12228491276356832373"
     },
     "user_tz": -60
    },
    "id": "VIkaEnbImimA",
    "outputId": "05dfde81-8d13-471f-96d9-80011752a0a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trening modelu dla splitu 20.0% danych adwersarialnych...\n",
      "\n",
      "### Rozpoczynam trenowanie dla splitu 20.0% ###\n",
      "\n",
      "Generowanie 7560 przykładów adwersarialnych...\n",
      "Od 0 do 63 z 7560\n",
      "Od 63 do 126 z 7560\n",
      "Od 126 do 189 z 7560\n",
      "Od 189 do 252 z 7560\n",
      "Od 252 do 315 z 7560\n",
      "Od 315 do 378 z 7560\n",
      "Od 378 do 441 z 7560\n",
      "Od 441 do 504 z 7560\n",
      "Od 504 do 567 z 7560\n",
      "Od 567 do 630 z 7560\n",
      "Od 630 do 693 z 7560\n",
      "Od 693 do 756 z 7560\n",
      "Od 756 do 819 z 7560\n",
      "Od 819 do 882 z 7560\n",
      "Od 882 do 945 z 7560\n",
      "Od 945 do 1008 z 7560\n",
      "Od 1008 do 1071 z 7560\n",
      "Od 1071 do 1134 z 7560\n",
      "Od 1134 do 1197 z 7560\n",
      "Od 1197 do 1260 z 7560\n",
      "Od 1260 do 1323 z 7560\n",
      "Od 1323 do 1386 z 7560\n",
      "Od 1386 do 1449 z 7560\n",
      "Od 1449 do 1512 z 7560\n",
      "Od 1512 do 1575 z 7560\n",
      "Od 1575 do 1638 z 7560\n",
      "Od 1638 do 1701 z 7560\n",
      "Od 1701 do 1764 z 7560\n",
      "Od 1764 do 1827 z 7560\n",
      "Od 1827 do 1890 z 7560\n",
      "Od 1890 do 1953 z 7560\n",
      "Od 1953 do 2016 z 7560\n",
      "Od 2016 do 2079 z 7560\n",
      "Od 2079 do 2142 z 7560\n",
      "Od 2142 do 2205 z 7560\n",
      "Od 2205 do 2268 z 7560\n",
      "Od 2268 do 2331 z 7560\n",
      "Od 2331 do 2394 z 7560\n",
      "Od 2394 do 2457 z 7560\n",
      "Od 2457 do 2520 z 7560\n",
      "Od 2520 do 2583 z 7560\n",
      "Od 2583 do 2646 z 7560\n",
      "Od 2646 do 2709 z 7560\n",
      "Od 2709 do 2772 z 7560\n",
      "Od 2772 do 2835 z 7560\n",
      "Od 2835 do 2898 z 7560\n",
      "Od 2898 do 2961 z 7560\n",
      "Od 2961 do 3024 z 7560\n",
      "Od 3024 do 3087 z 7560\n",
      "Od 3087 do 3150 z 7560\n",
      "Od 3150 do 3213 z 7560\n",
      "Od 3213 do 3276 z 7560\n",
      "Od 3276 do 3339 z 7560\n",
      "Od 3339 do 3402 z 7560\n",
      "Od 3402 do 3465 z 7560\n",
      "Od 3465 do 3528 z 7560\n",
      "Od 3528 do 3591 z 7560\n",
      "Od 3591 do 3654 z 7560\n",
      "Od 3654 do 3717 z 7560\n",
      "Od 3717 do 3780 z 7560\n",
      "Od 3780 do 3843 z 7560\n",
      "Od 3843 do 3906 z 7560\n",
      "Od 3906 do 3969 z 7560\n",
      "Od 3969 do 4032 z 7560\n",
      "Od 4032 do 4095 z 7560\n",
      "Od 4095 do 4158 z 7560\n",
      "Od 4158 do 4221 z 7560\n",
      "Od 4221 do 4284 z 7560\n",
      "Od 4284 do 4347 z 7560\n",
      "Od 4347 do 4410 z 7560\n",
      "Od 4410 do 4473 z 7560\n",
      "Od 4473 do 4536 z 7560\n",
      "Od 4536 do 4599 z 7560\n",
      "Od 4599 do 4662 z 7560\n",
      "Od 4662 do 4725 z 7560\n",
      "Od 4725 do 4788 z 7560\n",
      "Od 4788 do 4851 z 7560\n",
      "Od 4851 do 4914 z 7560\n",
      "Od 4914 do 4977 z 7560\n",
      "Od 4977 do 5040 z 7560\n",
      "Od 5040 do 5103 z 7560\n",
      "Od 5103 do 5166 z 7560\n",
      "Od 5166 do 5229 z 7560\n",
      "Od 5229 do 5292 z 7560\n",
      "Od 5292 do 5355 z 7560\n",
      "Od 5355 do 5418 z 7560\n",
      "Od 5418 do 5481 z 7560\n",
      "Od 5481 do 5544 z 7560\n",
      "Od 5544 do 5607 z 7560\n",
      "Od 5607 do 5670 z 7560\n",
      "Od 5670 do 5733 z 7560\n",
      "Od 5733 do 5796 z 7560\n",
      "Od 5796 do 5859 z 7560\n",
      "Od 5859 do 5922 z 7560\n",
      "Od 5922 do 5985 z 7560\n",
      "Od 5985 do 6048 z 7560\n",
      "Od 6048 do 6111 z 7560\n",
      "Od 6111 do 6174 z 7560\n",
      "Od 6174 do 6237 z 7560\n",
      "Od 6237 do 6300 z 7560\n",
      "Od 6300 do 6363 z 7560\n",
      "Od 6363 do 6426 z 7560\n",
      "Od 6426 do 6489 z 7560\n",
      "Od 6489 do 6552 z 7560\n",
      "Od 6552 do 6615 z 7560\n",
      "Od 6615 do 6678 z 7560\n",
      "Od 6678 do 6741 z 7560\n",
      "Od 6741 do 6804 z 7560\n",
      "Od 6804 do 6867 z 7560\n",
      "Od 6867 do 6930 z 7560\n",
      "Od 6930 do 6993 z 7560\n",
      "Od 6993 do 7056 z 7560\n",
      "Od 7056 do 7119 z 7560\n",
      "Od 7119 do 7182 z 7560\n",
      "Od 7182 do 7245 z 7560\n",
      "Od 7245 do 7308 z 7560\n",
      "Od 7308 do 7371 z 7560\n",
      "Od 7371 do 7434 z 7560\n",
      "Od 7434 do 7497 z 7560\n",
      "Od 7497 do 7560 z 7560\n",
      "Przykłady adwersarialne treningowe wygenerowane w czasie: 248.85 s\n",
      "Generowanie 7560 przykładów adwersarialnych...\n",
      "Od 0 do 42 z 840\n",
      "Od 42 do 84 z 840\n",
      "Od 84 do 126 z 840\n",
      "Od 126 do 168 z 840\n",
      "Od 168 do 210 z 840\n",
      "Od 210 do 252 z 840\n",
      "Od 252 do 294 z 840\n",
      "Od 294 do 336 z 840\n",
      "Od 336 do 378 z 840\n",
      "Od 378 do 420 z 840\n",
      "Od 420 do 462 z 840\n",
      "Od 462 do 504 z 840\n",
      "Od 504 do 546 z 840\n",
      "Od 546 do 588 z 840\n",
      "Od 588 do 630 z 840\n",
      "Od 630 do 672 z 840\n",
      "Od 672 do 714 z 840\n",
      "Od 714 do 756 z 840\n",
      "Od 756 do 798 z 840\n",
      "Od 798 do 840 z 840\n",
      "Łączenie danych oryginalnych i adwersarialnych...\n",
      "x_train_combined shape: (45360, 28, 28, 1)\n",
      "y_train_combined shape: (45360, 10)\n",
      "x_test_combined shape: (5040, 28, 28, 1)\n",
      "y_test_combined shape: (5040, 10)\n",
      "Rozpoczęcie trenowania modelu...\n",
      "Epoch 1/50\n",
      "\u001b[1m708/708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 34ms/step - accuracy: 0.7885 - loss: 0.7490 - val_accuracy: 0.8373 - val_loss: 2.3497 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m708/708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 923us/step - accuracy: 0.9531 - loss: 0.1503 - val_accuracy: 0.8379 - val_loss: 2.3599 - learning_rate: 9.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m708/708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 27ms/step - accuracy: 0.9585 - loss: 0.1339 - val_accuracy: 0.8429 - val_loss: 2.6908 - learning_rate: 8.1000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m708/708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step - accuracy: 0.9844 - loss: 0.0617 - val_accuracy: 0.8429 - val_loss: 2.6951 - learning_rate: 7.2900e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m708/708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 25ms/step - accuracy: 0.9713 - loss: 0.0979 - val_accuracy: 0.8419 - val_loss: 2.4814 - learning_rate: 6.5610e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m708/708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - accuracy: 1.0000 - loss: 0.0068 - val_accuracy: 0.8419 - val_loss: 2.4794 - learning_rate: 5.9049e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m708/708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 27ms/step - accuracy: 0.9776 - loss: 0.0749 - val_accuracy: 0.8433 - val_loss: 2.7750 - learning_rate: 5.3144e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m708/708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - accuracy: 0.9688 - loss: 0.0559 - val_accuracy: 0.8433 - val_loss: 2.7792 - learning_rate: 4.7830e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m708/708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 25ms/step - accuracy: 0.9815 - loss: 0.0615 - val_accuracy: 0.8452 - val_loss: 2.6948 - learning_rate: 4.3047e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m708/708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 503us/step - accuracy: 0.9844 - loss: 0.1242 - val_accuracy: 0.8452 - val_loss: 2.6891 - learning_rate: 3.8742e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m708/708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 27ms/step - accuracy: 0.9826 - loss: 0.0538 - val_accuracy: 0.8458 - val_loss: 2.7651 - learning_rate: 3.4868e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m708/708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541us/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.8458 - val_loss: 2.7599 - learning_rate: 3.1381e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m708/708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 26ms/step - accuracy: 0.9870 - loss: 0.0455 - val_accuracy: 0.8452 - val_loss: 2.7599 - learning_rate: 2.8243e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m708/708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 521us/step - accuracy: 0.9844 - loss: 0.0612 - val_accuracy: 0.8446 - val_loss: 2.7609 - learning_rate: 2.5419e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m708/708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 28ms/step - accuracy: 0.9881 - loss: 0.0407 - val_accuracy: 0.8464 - val_loss: 2.7218 - learning_rate: 2.2877e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m708/708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - accuracy: 0.9531 - loss: 0.1190 - val_accuracy: 0.8464 - val_loss: 2.7223 - learning_rate: 2.0589e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m708/708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 25ms/step - accuracy: 0.9880 - loss: 0.0401 - val_accuracy: 0.8470 - val_loss: 2.7824 - learning_rate: 1.8530e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m708/708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522us/step - accuracy: 0.9844 - loss: 0.0261 - val_accuracy: 0.8470 - val_loss: 2.7823 - learning_rate: 1.6677e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m708/708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 27ms/step - accuracy: 0.9897 - loss: 0.0338 - val_accuracy: 0.8468 - val_loss: 2.8441 - learning_rate: 1.5009e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m708/708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 927us/step - accuracy: 0.9688 - loss: 0.0721 - val_accuracy: 0.8470 - val_loss: 2.8462 - learning_rate: 1.3509e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m708/708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 28ms/step - accuracy: 0.9919 - loss: 0.0258 - val_accuracy: 0.8476 - val_loss: 2.9093 - learning_rate: 1.2158e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model wytrenowany dla splitu 20.0% w czasie: 532.16 s\n",
      "\n",
      "Model dla splitu 20.0% zapisany jako AD_model_PGD_0.2.h5\n",
      "Czas trwania treningu dla splitu 20.0%: 532.26 sekund\n",
      "\n",
      "Trening modelu dla splitu 30.0% danych adwersarialnych...\n",
      "\n",
      "### Rozpoczynam trenowanie dla splitu 30.0% ###\n",
      "\n",
      "Generowanie 11340 przykładów adwersarialnych...\n",
      "Od 0 do 63 z 11340\n",
      "Od 63 do 126 z 11340\n",
      "Od 126 do 189 z 11340\n",
      "Od 189 do 252 z 11340\n",
      "Od 252 do 315 z 11340\n",
      "Od 315 do 378 z 11340\n",
      "Od 378 do 441 z 11340\n",
      "Od 441 do 504 z 11340\n",
      "Od 504 do 567 z 11340\n",
      "Od 567 do 630 z 11340\n",
      "Od 630 do 693 z 11340\n",
      "Od 693 do 756 z 11340\n",
      "Od 756 do 819 z 11340\n",
      "Od 819 do 882 z 11340\n",
      "Od 882 do 945 z 11340\n",
      "Od 945 do 1008 z 11340\n",
      "Od 1008 do 1071 z 11340\n",
      "Od 1071 do 1134 z 11340\n",
      "Od 1134 do 1197 z 11340\n",
      "Od 1197 do 1260 z 11340\n",
      "Od 1260 do 1323 z 11340\n",
      "Od 1323 do 1386 z 11340\n",
      "Od 1386 do 1449 z 11340\n",
      "Od 1449 do 1512 z 11340\n",
      "Od 1512 do 1575 z 11340\n",
      "Od 1575 do 1638 z 11340\n",
      "Od 1638 do 1701 z 11340\n",
      "Od 1701 do 1764 z 11340\n",
      "Od 1764 do 1827 z 11340\n",
      "Od 1827 do 1890 z 11340\n",
      "Od 1890 do 1953 z 11340\n",
      "Od 1953 do 2016 z 11340\n",
      "Od 2016 do 2079 z 11340\n",
      "Od 2079 do 2142 z 11340\n",
      "Od 2142 do 2205 z 11340\n",
      "Od 2205 do 2268 z 11340\n",
      "Od 2268 do 2331 z 11340\n",
      "Od 2331 do 2394 z 11340\n",
      "Od 2394 do 2457 z 11340\n",
      "Od 2457 do 2520 z 11340\n",
      "Od 2520 do 2583 z 11340\n",
      "Od 2583 do 2646 z 11340\n",
      "Od 2646 do 2709 z 11340\n",
      "Od 2709 do 2772 z 11340\n",
      "Od 2772 do 2835 z 11340\n",
      "Od 2835 do 2898 z 11340\n",
      "Od 2898 do 2961 z 11340\n",
      "Od 2961 do 3024 z 11340\n",
      "Od 3024 do 3087 z 11340\n",
      "Od 3087 do 3150 z 11340\n",
      "Od 3150 do 3213 z 11340\n",
      "Od 3213 do 3276 z 11340\n",
      "Od 3276 do 3339 z 11340\n",
      "Od 3339 do 3402 z 11340\n",
      "Od 3402 do 3465 z 11340\n",
      "Od 3465 do 3528 z 11340\n",
      "Od 3528 do 3591 z 11340\n",
      "Od 3591 do 3654 z 11340\n",
      "Od 3654 do 3717 z 11340\n",
      "Od 3717 do 3780 z 11340\n",
      "Od 3780 do 3843 z 11340\n",
      "Od 3843 do 3906 z 11340\n",
      "Od 3906 do 3969 z 11340\n",
      "Od 3969 do 4032 z 11340\n",
      "Od 4032 do 4095 z 11340\n",
      "Od 4095 do 4158 z 11340\n",
      "Od 4158 do 4221 z 11340\n",
      "Od 4221 do 4284 z 11340\n",
      "Od 4284 do 4347 z 11340\n",
      "Od 4347 do 4410 z 11340\n",
      "Od 4410 do 4473 z 11340\n",
      "Od 4473 do 4536 z 11340\n",
      "Od 4536 do 4599 z 11340\n",
      "Od 4599 do 4662 z 11340\n",
      "Od 4662 do 4725 z 11340\n",
      "Od 4725 do 4788 z 11340\n",
      "Od 4788 do 4851 z 11340\n",
      "Od 4851 do 4914 z 11340\n",
      "Od 4914 do 4977 z 11340\n",
      "Od 4977 do 5040 z 11340\n",
      "Od 5040 do 5103 z 11340\n",
      "Od 5103 do 5166 z 11340\n",
      "Od 5166 do 5229 z 11340\n",
      "Od 5229 do 5292 z 11340\n",
      "Od 5292 do 5355 z 11340\n",
      "Od 5355 do 5418 z 11340\n",
      "Od 5418 do 5481 z 11340\n",
      "Od 5481 do 5544 z 11340\n",
      "Od 5544 do 5607 z 11340\n",
      "Od 5607 do 5670 z 11340\n",
      "Od 5670 do 5733 z 11340\n",
      "Od 5733 do 5796 z 11340\n",
      "Od 5796 do 5859 z 11340\n",
      "Od 5859 do 5922 z 11340\n",
      "Od 5922 do 5985 z 11340\n",
      "Od 5985 do 6048 z 11340\n",
      "Od 6048 do 6111 z 11340\n",
      "Od 6111 do 6174 z 11340\n",
      "Od 6174 do 6237 z 11340\n",
      "Od 6237 do 6300 z 11340\n",
      "Od 6300 do 6363 z 11340\n",
      "Od 6363 do 6426 z 11340\n",
      "Od 6426 do 6489 z 11340\n",
      "Od 6489 do 6552 z 11340\n",
      "Od 6552 do 6615 z 11340\n",
      "Od 6615 do 6678 z 11340\n",
      "Od 6678 do 6741 z 11340\n",
      "Od 6741 do 6804 z 11340\n",
      "Od 6804 do 6867 z 11340\n",
      "Od 6867 do 6930 z 11340\n",
      "Od 6930 do 6993 z 11340\n",
      "Od 6993 do 7056 z 11340\n",
      "Od 7056 do 7119 z 11340\n",
      "Od 7119 do 7182 z 11340\n",
      "Od 7182 do 7245 z 11340\n",
      "Od 7245 do 7308 z 11340\n",
      "Od 7308 do 7371 z 11340\n",
      "Od 7371 do 7434 z 11340\n",
      "Od 7434 do 7497 z 11340\n",
      "Od 7497 do 7560 z 11340\n",
      "Od 7560 do 7623 z 11340\n",
      "Od 7623 do 7686 z 11340\n",
      "Od 7686 do 7749 z 11340\n",
      "Od 7749 do 7812 z 11340\n",
      "Od 7812 do 7875 z 11340\n",
      "Od 7875 do 7938 z 11340\n",
      "Od 7938 do 8001 z 11340\n",
      "Od 8001 do 8064 z 11340\n",
      "Od 8064 do 8127 z 11340\n",
      "Od 8127 do 8190 z 11340\n",
      "Od 8190 do 8253 z 11340\n",
      "Od 8253 do 8316 z 11340\n",
      "Od 8316 do 8379 z 11340\n",
      "Od 8379 do 8442 z 11340\n",
      "Od 8442 do 8505 z 11340\n",
      "Od 8505 do 8568 z 11340\n",
      "Od 8568 do 8631 z 11340\n",
      "Od 8631 do 8694 z 11340\n",
      "Od 8694 do 8757 z 11340\n",
      "Od 8757 do 8820 z 11340\n",
      "Od 8820 do 8883 z 11340\n",
      "Od 8883 do 8946 z 11340\n",
      "Od 8946 do 9009 z 11340\n",
      "Od 9009 do 9072 z 11340\n",
      "Od 9072 do 9135 z 11340\n",
      "Od 9135 do 9198 z 11340\n",
      "Od 9198 do 9261 z 11340\n",
      "Od 9261 do 9324 z 11340\n",
      "Od 9324 do 9387 z 11340\n",
      "Od 9387 do 9450 z 11340\n",
      "Od 9450 do 9513 z 11340\n",
      "Od 9513 do 9576 z 11340\n",
      "Od 9576 do 9639 z 11340\n",
      "Od 9639 do 9702 z 11340\n",
      "Od 9702 do 9765 z 11340\n",
      "Od 9765 do 9828 z 11340\n",
      "Od 9828 do 9891 z 11340\n",
      "Od 9891 do 9954 z 11340\n",
      "Od 9954 do 10017 z 11340\n",
      "Od 10017 do 10080 z 11340\n",
      "Od 10080 do 10143 z 11340\n",
      "Od 10143 do 10206 z 11340\n",
      "Od 10206 do 10269 z 11340\n",
      "Od 10269 do 10332 z 11340\n",
      "Od 10332 do 10395 z 11340\n",
      "Od 10395 do 10458 z 11340\n",
      "Od 10458 do 10521 z 11340\n",
      "Od 10521 do 10584 z 11340\n",
      "Od 10584 do 10647 z 11340\n",
      "Od 10647 do 10710 z 11340\n",
      "Od 10710 do 10773 z 11340\n",
      "Od 10773 do 10836 z 11340\n",
      "Od 10836 do 10899 z 11340\n",
      "Od 10899 do 10962 z 11340\n",
      "Od 10962 do 11025 z 11340\n",
      "Od 11025 do 11088 z 11340\n",
      "Od 11088 do 11151 z 11340\n",
      "Od 11151 do 11214 z 11340\n",
      "Od 11214 do 11277 z 11340\n",
      "Od 11277 do 11340 z 11340\n",
      "Przykłady adwersarialne treningowe wygenerowane w czasie: 374.38 s\n",
      "Generowanie 11340 przykładów adwersarialnych...\n",
      "Od 0 do 42 z 1260\n",
      "Od 42 do 84 z 1260\n",
      "Od 84 do 126 z 1260\n",
      "Od 126 do 168 z 1260\n",
      "Od 168 do 210 z 1260\n",
      "Od 210 do 252 z 1260\n",
      "Od 252 do 294 z 1260\n",
      "Od 294 do 336 z 1260\n",
      "Od 336 do 378 z 1260\n",
      "Od 378 do 420 z 1260\n",
      "Od 420 do 462 z 1260\n",
      "Od 462 do 504 z 1260\n",
      "Od 504 do 546 z 1260\n",
      "Od 546 do 588 z 1260\n",
      "Od 588 do 630 z 1260\n",
      "Od 630 do 672 z 1260\n",
      "Od 672 do 714 z 1260\n",
      "Od 714 do 756 z 1260\n",
      "Od 756 do 798 z 1260\n",
      "Od 798 do 840 z 1260\n",
      "Od 840 do 882 z 1260\n",
      "Od 882 do 924 z 1260\n",
      "Od 924 do 966 z 1260\n",
      "Od 966 do 1008 z 1260\n",
      "Od 1008 do 1050 z 1260\n",
      "Od 1050 do 1092 z 1260\n",
      "Od 1092 do 1134 z 1260\n",
      "Od 1134 do 1176 z 1260\n",
      "Od 1176 do 1218 z 1260\n",
      "Od 1218 do 1260 z 1260\n",
      "Łączenie danych oryginalnych i adwersarialnych...\n",
      "x_train_combined shape: (49140, 28, 28, 1)\n",
      "y_train_combined shape: (49140, 10)\n",
      "x_test_combined shape: (5460, 28, 28, 1)\n",
      "y_test_combined shape: (5460, 10)\n",
      "Rozpoczęcie trenowania modelu...\n",
      "Epoch 1/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.8072 - loss: 0.6873 - val_accuracy: 0.7766 - val_loss: 3.2561 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520us/step - accuracy: 0.9688 - loss: 0.2129 - val_accuracy: 0.7767 - val_loss: 3.2538 - learning_rate: 9.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 27ms/step - accuracy: 0.9663 - loss: 0.1126 - val_accuracy: 0.7810 - val_loss: 3.4608 - learning_rate: 8.1000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 850us/step - accuracy: 0.9844 - loss: 0.0278 - val_accuracy: 0.7804 - val_loss: 3.4631 - learning_rate: 7.2900e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 27ms/step - accuracy: 0.9759 - loss: 0.0849 - val_accuracy: 0.7830 - val_loss: 3.4914 - learning_rate: 6.5610e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 843us/step - accuracy: 0.9844 - loss: 0.0382 - val_accuracy: 0.7832 - val_loss: 3.4905 - learning_rate: 5.9049e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 26ms/step - accuracy: 0.9796 - loss: 0.0659 - val_accuracy: 0.7839 - val_loss: 3.7762 - learning_rate: 5.3144e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 845us/step - accuracy: 0.9844 - loss: 0.1263 - val_accuracy: 0.7844 - val_loss: 3.7822 - learning_rate: 4.7830e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 26ms/step - accuracy: 0.9820 - loss: 0.0602 - val_accuracy: 0.7877 - val_loss: 3.9733 - learning_rate: 4.3047e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 860us/step - accuracy: 0.9688 - loss: 0.0291 - val_accuracy: 0.7877 - val_loss: 3.9765 - learning_rate: 3.8742e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.9846 - loss: 0.0493 - val_accuracy: 0.7879 - val_loss: 3.6910 - learning_rate: 3.4868e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 499us/step - accuracy: 1.0000 - loss: 0.0079 - val_accuracy: 0.7877 - val_loss: 3.6854 - learning_rate: 3.1381e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 26ms/step - accuracy: 0.9882 - loss: 0.0377 - val_accuracy: 0.7888 - val_loss: 4.0500 - learning_rate: 2.8243e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 845us/step - accuracy: 1.0000 - loss: 0.0027 - val_accuracy: 0.7888 - val_loss: 4.0541 - learning_rate: 2.5419e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 27ms/step - accuracy: 0.9887 - loss: 0.0393 - val_accuracy: 0.7877 - val_loss: 3.8668 - learning_rate: 2.2877e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 543us/step - accuracy: 1.0000 - loss: 0.0060 - val_accuracy: 0.7875 - val_loss: 3.8643 - learning_rate: 2.0589e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 27ms/step - accuracy: 0.9893 - loss: 0.0331 - val_accuracy: 0.7883 - val_loss: 4.1297 - learning_rate: 1.8530e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 626us/step - accuracy: 0.9844 - loss: 0.0489 - val_accuracy: 0.7883 - val_loss: 4.1238 - learning_rate: 1.6677e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 25ms/step - accuracy: 0.9892 - loss: 0.0348 - val_accuracy: 0.7899 - val_loss: 3.9145 - learning_rate: 1.5009e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 514us/step - accuracy: 1.0000 - loss: 0.0047 - val_accuracy: 0.7899 - val_loss: 3.9140 - learning_rate: 1.3509e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 27ms/step - accuracy: 0.9928 - loss: 0.0251 - val_accuracy: 0.7892 - val_loss: 3.9378 - learning_rate: 1.2158e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 510us/step - accuracy: 1.0000 - loss: 0.0195 - val_accuracy: 0.7892 - val_loss: 3.9419 - learning_rate: 1.0942e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model wytrenowany dla splitu 30.0% w czasie: 829.35 s\n",
      "\n",
      "Model dla splitu 30.0% zapisany jako AD_model_PGD_0.3.h5\n",
      "Czas trwania treningu dla splitu 30.0%: 829.45 sekund\n",
      "\n",
      "Trening modelu dla splitu 40.0% danych adwersarialnych...\n",
      "\n",
      "### Rozpoczynam trenowanie dla splitu 40.0% ###\n",
      "\n",
      "Generowanie 15120 przykładów adwersarialnych...\n",
      "Od 0 do 63 z 15120\n",
      "Od 63 do 126 z 15120\n",
      "Od 126 do 189 z 15120\n",
      "Od 189 do 252 z 15120\n",
      "Od 252 do 315 z 15120\n",
      "Od 315 do 378 z 15120\n",
      "Od 378 do 441 z 15120\n",
      "Od 441 do 504 z 15120\n",
      "Od 504 do 567 z 15120\n",
      "Od 567 do 630 z 15120\n",
      "Od 630 do 693 z 15120\n",
      "Od 693 do 756 z 15120\n",
      "Od 756 do 819 z 15120\n",
      "Od 819 do 882 z 15120\n",
      "Od 882 do 945 z 15120\n",
      "Od 945 do 1008 z 15120\n",
      "Od 1008 do 1071 z 15120\n",
      "Od 1071 do 1134 z 15120\n",
      "Od 1134 do 1197 z 15120\n",
      "Od 1197 do 1260 z 15120\n",
      "Od 1260 do 1323 z 15120\n",
      "Od 1323 do 1386 z 15120\n",
      "Od 1386 do 1449 z 15120\n",
      "Od 1449 do 1512 z 15120\n",
      "Od 1512 do 1575 z 15120\n",
      "Od 1575 do 1638 z 15120\n",
      "Od 1638 do 1701 z 15120\n",
      "Od 1701 do 1764 z 15120\n",
      "Od 1764 do 1827 z 15120\n",
      "Od 1827 do 1890 z 15120\n",
      "Od 1890 do 1953 z 15120\n",
      "Od 1953 do 2016 z 15120\n",
      "Od 2016 do 2079 z 15120\n",
      "Od 2079 do 2142 z 15120\n",
      "Od 2142 do 2205 z 15120\n",
      "Od 2205 do 2268 z 15120\n",
      "Od 2268 do 2331 z 15120\n",
      "Od 2331 do 2394 z 15120\n",
      "Od 2394 do 2457 z 15120\n",
      "Od 2457 do 2520 z 15120\n",
      "Od 2520 do 2583 z 15120\n",
      "Od 2583 do 2646 z 15120\n",
      "Od 2646 do 2709 z 15120\n",
      "Od 2709 do 2772 z 15120\n",
      "Od 2772 do 2835 z 15120\n",
      "Od 2835 do 2898 z 15120\n",
      "Od 2898 do 2961 z 15120\n",
      "Od 2961 do 3024 z 15120\n",
      "Od 3024 do 3087 z 15120\n",
      "Od 3087 do 3150 z 15120\n",
      "Od 3150 do 3213 z 15120\n",
      "Od 3213 do 3276 z 15120\n",
      "Od 3276 do 3339 z 15120\n",
      "Od 3339 do 3402 z 15120\n",
      "Od 3402 do 3465 z 15120\n",
      "Od 3465 do 3528 z 15120\n",
      "Od 3528 do 3591 z 15120\n",
      "Od 3591 do 3654 z 15120\n",
      "Od 3654 do 3717 z 15120\n",
      "Od 3717 do 3780 z 15120\n",
      "Od 3780 do 3843 z 15120\n",
      "Od 3843 do 3906 z 15120\n",
      "Od 3906 do 3969 z 15120\n",
      "Od 3969 do 4032 z 15120\n",
      "Od 4032 do 4095 z 15120\n",
      "Od 4095 do 4158 z 15120\n",
      "Od 4158 do 4221 z 15120\n",
      "Od 4221 do 4284 z 15120\n",
      "Od 4284 do 4347 z 15120\n",
      "Od 4347 do 4410 z 15120\n",
      "Od 4410 do 4473 z 15120\n",
      "Od 4473 do 4536 z 15120\n",
      "Od 4536 do 4599 z 15120\n",
      "Od 4599 do 4662 z 15120\n",
      "Od 4662 do 4725 z 15120\n",
      "Od 4725 do 4788 z 15120\n",
      "Od 4788 do 4851 z 15120\n",
      "Od 4851 do 4914 z 15120\n",
      "Od 4914 do 4977 z 15120\n",
      "Od 4977 do 5040 z 15120\n",
      "Od 5040 do 5103 z 15120\n",
      "Od 5103 do 5166 z 15120\n",
      "Od 5166 do 5229 z 15120\n",
      "Od 5229 do 5292 z 15120\n",
      "Od 5292 do 5355 z 15120\n",
      "Od 5355 do 5418 z 15120\n",
      "Od 5418 do 5481 z 15120\n",
      "Od 5481 do 5544 z 15120\n",
      "Od 5544 do 5607 z 15120\n",
      "Od 5607 do 5670 z 15120\n",
      "Od 5670 do 5733 z 15120\n",
      "Od 5733 do 5796 z 15120\n",
      "Od 5796 do 5859 z 15120\n",
      "Od 5859 do 5922 z 15120\n",
      "Od 5922 do 5985 z 15120\n",
      "Od 5985 do 6048 z 15120\n",
      "Od 6048 do 6111 z 15120\n",
      "Od 6111 do 6174 z 15120\n",
      "Od 6174 do 6237 z 15120\n",
      "Od 6237 do 6300 z 15120\n",
      "Od 6300 do 6363 z 15120\n",
      "Od 6363 do 6426 z 15120\n",
      "Od 6426 do 6489 z 15120\n",
      "Od 6489 do 6552 z 15120\n",
      "Od 6552 do 6615 z 15120\n",
      "Od 6615 do 6678 z 15120\n",
      "Od 6678 do 6741 z 15120\n",
      "Od 6741 do 6804 z 15120\n",
      "Od 6804 do 6867 z 15120\n",
      "Od 6867 do 6930 z 15120\n",
      "Od 6930 do 6993 z 15120\n",
      "Od 6993 do 7056 z 15120\n",
      "Od 7056 do 7119 z 15120\n",
      "Od 7119 do 7182 z 15120\n",
      "Od 7182 do 7245 z 15120\n",
      "Od 7245 do 7308 z 15120\n",
      "Od 7308 do 7371 z 15120\n",
      "Od 7371 do 7434 z 15120\n",
      "Od 7434 do 7497 z 15120\n",
      "Od 7497 do 7560 z 15120\n",
      "Od 7560 do 7623 z 15120\n",
      "Od 7623 do 7686 z 15120\n",
      "Od 7686 do 7749 z 15120\n",
      "Od 7749 do 7812 z 15120\n",
      "Od 7812 do 7875 z 15120\n",
      "Od 7875 do 7938 z 15120\n",
      "Od 7938 do 8001 z 15120\n",
      "Od 8001 do 8064 z 15120\n",
      "Od 8064 do 8127 z 15120\n",
      "Od 8127 do 8190 z 15120\n",
      "Od 8190 do 8253 z 15120\n",
      "Od 8253 do 8316 z 15120\n",
      "Od 8316 do 8379 z 15120\n",
      "Od 8379 do 8442 z 15120\n",
      "Od 8442 do 8505 z 15120\n",
      "Od 8505 do 8568 z 15120\n",
      "Od 8568 do 8631 z 15120\n",
      "Od 8631 do 8694 z 15120\n",
      "Od 8694 do 8757 z 15120\n",
      "Od 8757 do 8820 z 15120\n",
      "Od 8820 do 8883 z 15120\n",
      "Od 8883 do 8946 z 15120\n",
      "Od 8946 do 9009 z 15120\n",
      "Od 9009 do 9072 z 15120\n",
      "Od 9072 do 9135 z 15120\n",
      "Od 9135 do 9198 z 15120\n",
      "Od 9198 do 9261 z 15120\n",
      "Od 9261 do 9324 z 15120\n",
      "Od 9324 do 9387 z 15120\n",
      "Od 9387 do 9450 z 15120\n",
      "Od 9450 do 9513 z 15120\n",
      "Od 9513 do 9576 z 15120\n",
      "Od 9576 do 9639 z 15120\n",
      "Od 9639 do 9702 z 15120\n",
      "Od 9702 do 9765 z 15120\n",
      "Od 9765 do 9828 z 15120\n",
      "Od 9828 do 9891 z 15120\n",
      "Od 9891 do 9954 z 15120\n",
      "Od 9954 do 10017 z 15120\n",
      "Od 10017 do 10080 z 15120\n",
      "Od 10080 do 10143 z 15120\n",
      "Od 10143 do 10206 z 15120\n",
      "Od 10206 do 10269 z 15120\n",
      "Od 10269 do 10332 z 15120\n",
      "Od 10332 do 10395 z 15120\n",
      "Od 10395 do 10458 z 15120\n",
      "Od 10458 do 10521 z 15120\n",
      "Od 10521 do 10584 z 15120\n",
      "Od 10584 do 10647 z 15120\n",
      "Od 10647 do 10710 z 15120\n",
      "Od 10710 do 10773 z 15120\n",
      "Od 10773 do 10836 z 15120\n",
      "Od 10836 do 10899 z 15120\n",
      "Od 10899 do 10962 z 15120\n",
      "Od 10962 do 11025 z 15120\n",
      "Od 11025 do 11088 z 15120\n",
      "Od 11088 do 11151 z 15120\n",
      "Od 11151 do 11214 z 15120\n",
      "Od 11214 do 11277 z 15120\n",
      "Od 11277 do 11340 z 15120\n",
      "Od 11340 do 11403 z 15120\n",
      "Od 11403 do 11466 z 15120\n",
      "Od 11466 do 11529 z 15120\n",
      "Od 11529 do 11592 z 15120\n",
      "Od 11592 do 11655 z 15120\n",
      "Od 11655 do 11718 z 15120\n",
      "Od 11718 do 11781 z 15120\n",
      "Od 11781 do 11844 z 15120\n",
      "Od 11844 do 11907 z 15120\n",
      "Od 11907 do 11970 z 15120\n",
      "Od 11970 do 12033 z 15120\n",
      "Od 12033 do 12096 z 15120\n",
      "Od 12096 do 12159 z 15120\n",
      "Od 12159 do 12222 z 15120\n",
      "Od 12222 do 12285 z 15120\n",
      "Od 12285 do 12348 z 15120\n",
      "Od 12348 do 12411 z 15120\n",
      "Od 12411 do 12474 z 15120\n",
      "Od 12474 do 12537 z 15120\n",
      "Od 12537 do 12600 z 15120\n",
      "Od 12600 do 12663 z 15120\n",
      "Od 12663 do 12726 z 15120\n",
      "Od 12726 do 12789 z 15120\n",
      "Od 12789 do 12852 z 15120\n",
      "Od 12852 do 12915 z 15120\n",
      "Od 12915 do 12978 z 15120\n",
      "Od 12978 do 13041 z 15120\n",
      "Od 13041 do 13104 z 15120\n",
      "Od 13104 do 13167 z 15120\n",
      "Od 13167 do 13230 z 15120\n",
      "Od 13230 do 13293 z 15120\n",
      "Od 13293 do 13356 z 15120\n",
      "Od 13356 do 13419 z 15120\n",
      "Od 13419 do 13482 z 15120\n",
      "Od 13482 do 13545 z 15120\n",
      "Od 13545 do 13608 z 15120\n",
      "Od 13608 do 13671 z 15120\n",
      "Od 13671 do 13734 z 15120\n",
      "Od 13734 do 13797 z 15120\n",
      "Od 13797 do 13860 z 15120\n",
      "Od 13860 do 13923 z 15120\n",
      "Od 13923 do 13986 z 15120\n",
      "Od 13986 do 14049 z 15120\n",
      "Od 14049 do 14112 z 15120\n",
      "Od 14112 do 14175 z 15120\n",
      "Od 14175 do 14238 z 15120\n",
      "Od 14238 do 14301 z 15120\n",
      "Od 14301 do 14364 z 15120\n",
      "Od 14364 do 14427 z 15120\n",
      "Od 14427 do 14490 z 15120\n",
      "Od 14490 do 14553 z 15120\n",
      "Od 14553 do 14616 z 15120\n",
      "Od 14616 do 14679 z 15120\n",
      "Od 14679 do 14742 z 15120\n",
      "Od 14742 do 14805 z 15120\n",
      "Od 14805 do 14868 z 15120\n",
      "Od 14868 do 14931 z 15120\n",
      "Od 14931 do 14994 z 15120\n",
      "Od 14994 do 15057 z 15120\n",
      "Od 15057 do 15120 z 15120\n",
      "Przykłady adwersarialne treningowe wygenerowane w czasie: 491.32 s\n",
      "Generowanie 15120 przykładów adwersarialnych...\n",
      "Od 0 do 42 z 1680\n",
      "Od 42 do 84 z 1680\n",
      "Od 84 do 126 z 1680\n",
      "Od 126 do 168 z 1680\n",
      "Od 168 do 210 z 1680\n",
      "Od 210 do 252 z 1680\n",
      "Od 252 do 294 z 1680\n",
      "Od 294 do 336 z 1680\n",
      "Od 336 do 378 z 1680\n",
      "Od 378 do 420 z 1680\n",
      "Od 420 do 462 z 1680\n",
      "Od 462 do 504 z 1680\n",
      "Od 504 do 546 z 1680\n",
      "Od 546 do 588 z 1680\n",
      "Od 588 do 630 z 1680\n",
      "Od 630 do 672 z 1680\n",
      "Od 672 do 714 z 1680\n",
      "Od 714 do 756 z 1680\n",
      "Od 756 do 798 z 1680\n",
      "Od 798 do 840 z 1680\n",
      "Od 840 do 882 z 1680\n",
      "Od 882 do 924 z 1680\n",
      "Od 924 do 966 z 1680\n",
      "Od 966 do 1008 z 1680\n",
      "Od 1008 do 1050 z 1680\n",
      "Od 1050 do 1092 z 1680\n",
      "Od 1092 do 1134 z 1680\n",
      "Od 1134 do 1176 z 1680\n",
      "Od 1176 do 1218 z 1680\n",
      "Od 1218 do 1260 z 1680\n",
      "Od 1260 do 1302 z 1680\n",
      "Od 1302 do 1344 z 1680\n",
      "Od 1344 do 1386 z 1680\n",
      "Od 1386 do 1428 z 1680\n",
      "Od 1428 do 1470 z 1680\n",
      "Od 1470 do 1512 z 1680\n",
      "Od 1512 do 1554 z 1680\n",
      "Od 1554 do 1596 z 1680\n",
      "Od 1596 do 1638 z 1680\n",
      "Od 1638 do 1680 z 1680\n",
      "Łączenie danych oryginalnych i adwersarialnych...\n",
      "x_train_combined shape: (52920, 28, 28, 1)\n",
      "y_train_combined shape: (52920, 10)\n",
      "x_test_combined shape: (5880, 28, 28, 1)\n",
      "y_test_combined shape: (5880, 10)\n",
      "Rozpoczęcie trenowania modelu...\n",
      "Epoch 1/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 35ms/step - accuracy: 0.8136 - loss: 0.6662 - val_accuracy: 0.7286 - val_loss: 4.6568 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 543us/step - accuracy: 0.9375 - loss: 0.1594 - val_accuracy: 0.7303 - val_loss: 4.6343 - learning_rate: 9.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 25ms/step - accuracy: 0.9642 - loss: 0.1141 - val_accuracy: 0.7332 - val_loss: 5.0468 - learning_rate: 8.1000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 785us/step - accuracy: 0.9688 - loss: 0.0464 - val_accuracy: 0.7332 - val_loss: 5.0503 - learning_rate: 7.2900e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 27ms/step - accuracy: 0.9759 - loss: 0.0779 - val_accuracy: 0.7355 - val_loss: 5.2524 - learning_rate: 6.5610e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518us/step - accuracy: 0.9844 - loss: 0.0609 - val_accuracy: 0.7349 - val_loss: 5.3011 - learning_rate: 5.9049e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 29ms/step - accuracy: 0.9806 - loss: 0.0635 - val_accuracy: 0.7366 - val_loss: 4.9019 - learning_rate: 5.3144e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 515us/step - accuracy: 0.9531 - loss: 0.1072 - val_accuracy: 0.7359 - val_loss: 4.9263 - learning_rate: 4.7830e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 27ms/step - accuracy: 0.9840 - loss: 0.0516 - val_accuracy: 0.7349 - val_loss: 5.2406 - learning_rate: 4.3047e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 517us/step - accuracy: 0.9844 - loss: 0.1222 - val_accuracy: 0.7349 - val_loss: 5.2448 - learning_rate: 3.8742e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 28ms/step - accuracy: 0.9868 - loss: 0.0439 - val_accuracy: 0.7364 - val_loss: 5.0690 - learning_rate: 3.4868e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 506us/step - accuracy: 0.9844 - loss: 0.0220 - val_accuracy: 0.7366 - val_loss: 5.0860 - learning_rate: 3.1381e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 27ms/step - accuracy: 0.9882 - loss: 0.0385 - val_accuracy: 0.7372 - val_loss: 5.3538 - learning_rate: 2.8243e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 496us/step - accuracy: 0.9844 - loss: 0.0537 - val_accuracy: 0.7372 - val_loss: 5.3438 - learning_rate: 2.5419e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.9885 - loss: 0.0376 - val_accuracy: 0.7366 - val_loss: 5.6406 - learning_rate: 2.2877e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 789us/step - accuracy: 1.0000 - loss: 0.0063 - val_accuracy: 0.7367 - val_loss: 5.6379 - learning_rate: 2.0589e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 27ms/step - accuracy: 0.9904 - loss: 0.0307 - val_accuracy: 0.7383 - val_loss: 5.3446 - learning_rate: 1.8530e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 510us/step - accuracy: 1.0000 - loss: 7.5399e-04 - val_accuracy: 0.7383 - val_loss: 5.3413 - learning_rate: 1.6677e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 27ms/step - accuracy: 0.9901 - loss: 0.0316 - val_accuracy: 0.7389 - val_loss: 5.1916 - learning_rate: 1.5009e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 513us/step - accuracy: 0.9531 - loss: 0.0812 - val_accuracy: 0.7389 - val_loss: 5.1894 - learning_rate: 1.3509e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 27ms/step - accuracy: 0.9907 - loss: 0.0283 - val_accuracy: 0.7388 - val_loss: 5.5997 - learning_rate: 1.2158e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 657us/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.7388 - val_loss: 5.5985 - learning_rate: 1.0942e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model wytrenowany dla splitu 40.0% w czasie: 951.10 s\n",
      "\n",
      "Model dla splitu 40.0% zapisany jako AD_model_PGD_0.4.h5\n",
      "Czas trwania treningu dla splitu 40.0%: 951.21 sekund\n",
      "\n",
      "Wszystkie modele zostały wytrenowane i zapisane.\n"
     ]
    }
   ],
   "source": [
    "# Train and Save Models\n",
    "for split in splits:\n",
    "    start_time = time.time()\n",
    "    print(f\"\\nTrening modelu dla splitu {split * 100}% danych adwersarialnych...\")\n",
    "\n",
    "    # Trenowanie modelu\n",
    "    model = adversarial_training(x_train, y_train, model, x_test, y_test, batch_size, epochs, split=split, attack_type = \"PGD\", c = 0.3)\n",
    "\n",
    "    # Nazwa pliku\n",
    "    model_filename = f\"AD_model_PGD_{split:.1f}.h5\"\n",
    "\n",
    "    # Zapis modelu w bieżącym katalogu\n",
    "    model.save(model_filename)\n",
    "    print(f\"Model dla splitu {split * 100}% zapisany jako {model_filename}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    iteration_time = end_time - start_time\n",
    "    print(f\"Czas trwania treningu dla splitu {split * 100}%: {iteration_time:.2f} sekund\")\n",
    "\n",
    "print(\"\\nWszystkie modele zostały wytrenowane i zapisane.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2344243,
     "status": "ok",
     "timestamp": 1733862001072,
     "user": {
      "displayName": "Escooobar",
      "userId": "12228491276356832373"
     },
     "user_tz": -60
    },
    "id": "wqLXLUpJvRc1",
    "outputId": "cc5b60ae-4c98-42c7-e98a-41b7c23cdb6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trening modelu dla splitu 20.0% danych adwersarialnych...\n",
      "\n",
      "### Rozpoczynam trenowanie dla splitu 20.0% ###\n",
      "\n",
      "Generowanie 7560 przykładów adwersarialnych...\n",
      "Od 0 do 63 z 7560\n",
      "Od 63 do 126 z 7560\n",
      "Od 126 do 189 z 7560\n",
      "Od 189 do 252 z 7560\n",
      "Od 252 do 315 z 7560\n",
      "Od 315 do 378 z 7560\n",
      "Od 378 do 441 z 7560\n",
      "Od 441 do 504 z 7560\n",
      "Od 504 do 567 z 7560\n",
      "Od 567 do 630 z 7560\n",
      "Od 630 do 693 z 7560\n",
      "Od 693 do 756 z 7560\n",
      "Od 756 do 819 z 7560\n",
      "Od 819 do 882 z 7560\n",
      "Od 882 do 945 z 7560\n",
      "Od 945 do 1008 z 7560\n",
      "Od 1008 do 1071 z 7560\n",
      "Od 1071 do 1134 z 7560\n",
      "Od 1134 do 1197 z 7560\n",
      "Od 1197 do 1260 z 7560\n",
      "Od 1260 do 1323 z 7560\n",
      "Od 1323 do 1386 z 7560\n",
      "Od 1386 do 1449 z 7560\n",
      "Od 1449 do 1512 z 7560\n",
      "Od 1512 do 1575 z 7560\n",
      "Od 1575 do 1638 z 7560\n",
      "Od 1638 do 1701 z 7560\n",
      "Od 1701 do 1764 z 7560\n",
      "Od 1764 do 1827 z 7560\n",
      "Od 1827 do 1890 z 7560\n",
      "Od 1890 do 1953 z 7560\n",
      "Od 1953 do 2016 z 7560\n",
      "Od 2016 do 2079 z 7560\n",
      "Od 2079 do 2142 z 7560\n",
      "Od 2142 do 2205 z 7560\n",
      "Od 2205 do 2268 z 7560\n",
      "Od 2268 do 2331 z 7560\n",
      "Od 2331 do 2394 z 7560\n",
      "Od 2394 do 2457 z 7560\n",
      "Od 2457 do 2520 z 7560\n",
      "Od 2520 do 2583 z 7560\n",
      "Od 2583 do 2646 z 7560\n",
      "Od 2646 do 2709 z 7560\n",
      "Od 2709 do 2772 z 7560\n",
      "Od 2772 do 2835 z 7560\n",
      "Od 2835 do 2898 z 7560\n",
      "Od 2898 do 2961 z 7560\n",
      "Od 2961 do 3024 z 7560\n",
      "Od 3024 do 3087 z 7560\n",
      "Od 3087 do 3150 z 7560\n",
      "Od 3150 do 3213 z 7560\n",
      "Od 3213 do 3276 z 7560\n",
      "Od 3276 do 3339 z 7560\n",
      "Od 3339 do 3402 z 7560\n",
      "Od 3402 do 3465 z 7560\n",
      "Od 3465 do 3528 z 7560\n",
      "Od 3528 do 3591 z 7560\n",
      "Od 3591 do 3654 z 7560\n",
      "Od 3654 do 3717 z 7560\n",
      "Od 3717 do 3780 z 7560\n",
      "Od 3780 do 3843 z 7560\n",
      "Od 3843 do 3906 z 7560\n",
      "Od 3906 do 3969 z 7560\n",
      "Od 3969 do 4032 z 7560\n",
      "Od 4032 do 4095 z 7560\n",
      "Od 4095 do 4158 z 7560\n",
      "Od 4158 do 4221 z 7560\n",
      "Od 4221 do 4284 z 7560\n",
      "Od 4284 do 4347 z 7560\n",
      "Od 4347 do 4410 z 7560\n",
      "Od 4410 do 4473 z 7560\n",
      "Od 4473 do 4536 z 7560\n",
      "Od 4536 do 4599 z 7560\n",
      "Od 4599 do 4662 z 7560\n",
      "Od 4662 do 4725 z 7560\n",
      "Od 4725 do 4788 z 7560\n",
      "Od 4788 do 4851 z 7560\n",
      "Od 4851 do 4914 z 7560\n",
      "Od 4914 do 4977 z 7560\n",
      "Od 4977 do 5040 z 7560\n",
      "Od 5040 do 5103 z 7560\n",
      "Od 5103 do 5166 z 7560\n",
      "Od 5166 do 5229 z 7560\n",
      "Od 5229 do 5292 z 7560\n",
      "Od 5292 do 5355 z 7560\n",
      "Od 5355 do 5418 z 7560\n",
      "Od 5418 do 5481 z 7560\n",
      "Od 5481 do 5544 z 7560\n",
      "Od 5544 do 5607 z 7560\n",
      "Od 5607 do 5670 z 7560\n",
      "Od 5670 do 5733 z 7560\n",
      "Od 5733 do 5796 z 7560\n",
      "Od 5796 do 5859 z 7560\n",
      "Od 5859 do 5922 z 7560\n",
      "Od 5922 do 5985 z 7560\n",
      "Od 5985 do 6048 z 7560\n",
      "Od 6048 do 6111 z 7560\n",
      "Od 6111 do 6174 z 7560\n",
      "Od 6174 do 6237 z 7560\n",
      "Od 6237 do 6300 z 7560\n",
      "Od 6300 do 6363 z 7560\n",
      "Od 6363 do 6426 z 7560\n",
      "Od 6426 do 6489 z 7560\n",
      "Od 6489 do 6552 z 7560\n",
      "Od 6552 do 6615 z 7560\n",
      "Od 6615 do 6678 z 7560\n",
      "Od 6678 do 6741 z 7560\n",
      "Od 6741 do 6804 z 7560\n",
      "Od 6804 do 6867 z 7560\n",
      "Od 6867 do 6930 z 7560\n",
      "Od 6930 do 6993 z 7560\n",
      "Od 6993 do 7056 z 7560\n",
      "Od 7056 do 7119 z 7560\n",
      "Od 7119 do 7182 z 7560\n",
      "Od 7182 do 7245 z 7560\n",
      "Od 7245 do 7308 z 7560\n",
      "Od 7308 do 7371 z 7560\n",
      "Od 7371 do 7434 z 7560\n",
      "Od 7434 do 7497 z 7560\n",
      "Od 7497 do 7560 z 7560\n",
      "Przykłady adwersarialne treningowe wygenerowane w czasie: 292.16 s\n",
      "Generowanie 7560 przykładów adwersarialnych...\n",
      "Od 0 do 42 z 840\n",
      "Od 42 do 84 z 840\n",
      "Od 84 do 126 z 840\n",
      "Od 126 do 168 z 840\n",
      "Od 168 do 210 z 840\n",
      "Od 210 do 252 z 840\n",
      "Od 252 do 294 z 840\n",
      "Od 294 do 336 z 840\n",
      "Od 336 do 378 z 840\n",
      "Od 378 do 420 z 840\n",
      "Od 420 do 462 z 840\n",
      "Od 462 do 504 z 840\n",
      "Od 504 do 546 z 840\n",
      "Od 546 do 588 z 840\n",
      "Od 588 do 630 z 840\n",
      "Od 630 do 672 z 840\n",
      "Od 672 do 714 z 840\n",
      "Od 714 do 756 z 840\n",
      "Od 756 do 798 z 840\n",
      "Od 798 do 840 z 840\n",
      "Łączenie danych oryginalnych i adwersarialnych...\n",
      "x_train_combined shape: (45360, 28, 28, 1)\n",
      "y_train_combined shape: (45360, 10)\n",
      "x_test_combined shape: (5040, 28, 28, 1)\n",
      "y_test_combined shape: (5040, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozpoczęcie trenowania modelu...\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m708/708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 37ms/step - accuracy: 0.7832 - loss: 0.7707 - val_accuracy: 0.8331 - val_loss: 1.9411 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m  1/708\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - accuracy: 0.9375 - loss: 0.1409"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m708/708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 474us/step - accuracy: 0.9375 - loss: 0.1409 - val_accuracy: 0.8335 - val_loss: 1.9514 - learning_rate: 9.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m708/708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 26ms/step - accuracy: 0.9586 - loss: 0.1338 - val_accuracy: 0.8415 - val_loss: 2.0438 - learning_rate: 8.1000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m708/708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 940us/step - accuracy: 0.9531 - loss: 0.1611 - val_accuracy: 0.8419 - val_loss: 2.0302 - learning_rate: 7.2900e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m708/708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.9700 - loss: 0.0959 - val_accuracy: 0.8440 - val_loss: 2.1601 - learning_rate: 6.5610e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m708/708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 506us/step - accuracy: 0.9375 - loss: 0.2546 - val_accuracy: 0.8444 - val_loss: 2.1576 - learning_rate: 5.9049e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m708/708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 30ms/step - accuracy: 0.9763 - loss: 0.0746 - val_accuracy: 0.8444 - val_loss: 2.0275 - learning_rate: 5.3144e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m708/708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 917us/step - accuracy: 0.9844 - loss: 0.0256 - val_accuracy: 0.8444 - val_loss: 2.0322 - learning_rate: 4.7830e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m708/708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 26ms/step - accuracy: 0.9804 - loss: 0.0625 - val_accuracy: 0.8448 - val_loss: 2.2494 - learning_rate: 4.3047e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m708/708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - accuracy: 0.9844 - loss: 0.0304 - val_accuracy: 0.8448 - val_loss: 2.2478 - learning_rate: 3.8742e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m708/708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 27ms/step - accuracy: 0.9843 - loss: 0.0525 - val_accuracy: 0.8472 - val_loss: 2.2831 - learning_rate: 3.4868e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m708/708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 919us/step - accuracy: 0.9844 - loss: 0.0375 - val_accuracy: 0.8472 - val_loss: 2.2804 - learning_rate: 3.1381e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m708/708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 26ms/step - accuracy: 0.9849 - loss: 0.0490 - val_accuracy: 0.8450 - val_loss: 2.3486 - learning_rate: 2.8243e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m708/708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528us/step - accuracy: 0.9688 - loss: 0.0848 - val_accuracy: 0.8452 - val_loss: 2.3495 - learning_rate: 2.5419e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m708/708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 28ms/step - accuracy: 0.9868 - loss: 0.0460 - val_accuracy: 0.8458 - val_loss: 2.3797 - learning_rate: 2.2877e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m708/708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528us/step - accuracy: 0.9844 - loss: 0.0276 - val_accuracy: 0.8458 - val_loss: 2.3815 - learning_rate: 2.0589e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m708/708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 27ms/step - accuracy: 0.9882 - loss: 0.0387 - val_accuracy: 0.8468 - val_loss: 2.2944 - learning_rate: 1.8530e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m708/708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 917us/step - accuracy: 1.0000 - loss: 0.0051 - val_accuracy: 0.8468 - val_loss: 2.2885 - learning_rate: 1.6677e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m708/708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 28ms/step - accuracy: 0.9885 - loss: 0.0349 - val_accuracy: 0.8466 - val_loss: 2.4371 - learning_rate: 1.5009e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m708/708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 919us/step - accuracy: 0.9844 - loss: 0.0461 - val_accuracy: 0.8466 - val_loss: 2.4374 - learning_rate: 1.3509e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m708/708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 26ms/step - accuracy: 0.9897 - loss: 0.0322 - val_accuracy: 0.8466 - val_loss: 2.4898 - learning_rate: 1.2158e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model wytrenowany dla splitu 20.0% w czasie: 600.22 s\n",
      "\n",
      "Model dla splitu 20.0% zapisany jako AD_model_CW_0.2.h5\n",
      "Czas trwania treningu dla splitu 20.0%: 600.33 sekund\n",
      "\n",
      "Trening modelu dla splitu 30.0% danych adwersarialnych...\n",
      "\n",
      "### Rozpoczynam trenowanie dla splitu 30.0% ###\n",
      "\n",
      "Generowanie 11340 przykładów adwersarialnych...\n",
      "Od 0 do 63 z 11340\n",
      "Od 63 do 126 z 11340\n",
      "Od 126 do 189 z 11340\n",
      "Od 189 do 252 z 11340\n",
      "Od 252 do 315 z 11340\n",
      "Od 315 do 378 z 11340\n",
      "Od 378 do 441 z 11340\n",
      "Od 441 do 504 z 11340\n",
      "Od 504 do 567 z 11340\n",
      "Od 567 do 630 z 11340\n",
      "Od 630 do 693 z 11340\n",
      "Od 693 do 756 z 11340\n",
      "Od 756 do 819 z 11340\n",
      "Od 819 do 882 z 11340\n",
      "Od 882 do 945 z 11340\n",
      "Od 945 do 1008 z 11340\n",
      "Od 1008 do 1071 z 11340\n",
      "Od 1071 do 1134 z 11340\n",
      "Od 1134 do 1197 z 11340\n",
      "Od 1197 do 1260 z 11340\n",
      "Od 1260 do 1323 z 11340\n",
      "Od 1323 do 1386 z 11340\n",
      "Od 1386 do 1449 z 11340\n",
      "Od 1449 do 1512 z 11340\n",
      "Od 1512 do 1575 z 11340\n",
      "Od 1575 do 1638 z 11340\n",
      "Od 1638 do 1701 z 11340\n",
      "Od 1701 do 1764 z 11340\n",
      "Od 1764 do 1827 z 11340\n",
      "Od 1827 do 1890 z 11340\n",
      "Od 1890 do 1953 z 11340\n",
      "Od 1953 do 2016 z 11340\n",
      "Od 2016 do 2079 z 11340\n",
      "Od 2079 do 2142 z 11340\n",
      "Od 2142 do 2205 z 11340\n",
      "Od 2205 do 2268 z 11340\n",
      "Od 2268 do 2331 z 11340\n",
      "Od 2331 do 2394 z 11340\n",
      "Od 2394 do 2457 z 11340\n",
      "Od 2457 do 2520 z 11340\n",
      "Od 2520 do 2583 z 11340\n",
      "Od 2583 do 2646 z 11340\n",
      "Od 2646 do 2709 z 11340\n",
      "Od 2709 do 2772 z 11340\n",
      "Od 2772 do 2835 z 11340\n",
      "Od 2835 do 2898 z 11340\n",
      "Od 2898 do 2961 z 11340\n",
      "Od 2961 do 3024 z 11340\n",
      "Od 3024 do 3087 z 11340\n",
      "Od 3087 do 3150 z 11340\n",
      "Od 3150 do 3213 z 11340\n",
      "Od 3213 do 3276 z 11340\n",
      "Od 3276 do 3339 z 11340\n",
      "Od 3339 do 3402 z 11340\n",
      "Od 3402 do 3465 z 11340\n",
      "Od 3465 do 3528 z 11340\n",
      "Od 3528 do 3591 z 11340\n",
      "Od 3591 do 3654 z 11340\n",
      "Od 3654 do 3717 z 11340\n",
      "Od 3717 do 3780 z 11340\n",
      "Od 3780 do 3843 z 11340\n",
      "Od 3843 do 3906 z 11340\n",
      "Od 3906 do 3969 z 11340\n",
      "Od 3969 do 4032 z 11340\n",
      "Od 4032 do 4095 z 11340\n",
      "Od 4095 do 4158 z 11340\n",
      "Od 4158 do 4221 z 11340\n",
      "Od 4221 do 4284 z 11340\n",
      "Od 4284 do 4347 z 11340\n",
      "Od 4347 do 4410 z 11340\n",
      "Od 4410 do 4473 z 11340\n",
      "Od 4473 do 4536 z 11340\n",
      "Od 4536 do 4599 z 11340\n",
      "Od 4599 do 4662 z 11340\n",
      "Od 4662 do 4725 z 11340\n",
      "Od 4725 do 4788 z 11340\n",
      "Od 4788 do 4851 z 11340\n",
      "Od 4851 do 4914 z 11340\n",
      "Od 4914 do 4977 z 11340\n",
      "Od 4977 do 5040 z 11340\n",
      "Od 5040 do 5103 z 11340\n",
      "Od 5103 do 5166 z 11340\n",
      "Od 5166 do 5229 z 11340\n",
      "Od 5229 do 5292 z 11340\n",
      "Od 5292 do 5355 z 11340\n",
      "Od 5355 do 5418 z 11340\n",
      "Od 5418 do 5481 z 11340\n",
      "Od 5481 do 5544 z 11340\n",
      "Od 5544 do 5607 z 11340\n",
      "Od 5607 do 5670 z 11340\n",
      "Od 5670 do 5733 z 11340\n",
      "Od 5733 do 5796 z 11340\n",
      "Od 5796 do 5859 z 11340\n",
      "Od 5859 do 5922 z 11340\n",
      "Od 5922 do 5985 z 11340\n",
      "Od 5985 do 6048 z 11340\n",
      "Od 6048 do 6111 z 11340\n",
      "Od 6111 do 6174 z 11340\n",
      "Od 6174 do 6237 z 11340\n",
      "Od 6237 do 6300 z 11340\n",
      "Od 6300 do 6363 z 11340\n",
      "Od 6363 do 6426 z 11340\n",
      "Od 6426 do 6489 z 11340\n",
      "Od 6489 do 6552 z 11340\n",
      "Od 6552 do 6615 z 11340\n",
      "Od 6615 do 6678 z 11340\n",
      "Od 6678 do 6741 z 11340\n",
      "Od 6741 do 6804 z 11340\n",
      "Od 6804 do 6867 z 11340\n",
      "Od 6867 do 6930 z 11340\n",
      "Od 6930 do 6993 z 11340\n",
      "Od 6993 do 7056 z 11340\n",
      "Od 7056 do 7119 z 11340\n",
      "Od 7119 do 7182 z 11340\n",
      "Od 7182 do 7245 z 11340\n",
      "Od 7245 do 7308 z 11340\n",
      "Od 7308 do 7371 z 11340\n",
      "Od 7371 do 7434 z 11340\n",
      "Od 7434 do 7497 z 11340\n",
      "Od 7497 do 7560 z 11340\n",
      "Od 7560 do 7623 z 11340\n",
      "Od 7623 do 7686 z 11340\n",
      "Od 7686 do 7749 z 11340\n",
      "Od 7749 do 7812 z 11340\n",
      "Od 7812 do 7875 z 11340\n",
      "Od 7875 do 7938 z 11340\n",
      "Od 7938 do 8001 z 11340\n",
      "Od 8001 do 8064 z 11340\n",
      "Od 8064 do 8127 z 11340\n",
      "Od 8127 do 8190 z 11340\n",
      "Od 8190 do 8253 z 11340\n",
      "Od 8253 do 8316 z 11340\n",
      "Od 8316 do 8379 z 11340\n",
      "Od 8379 do 8442 z 11340\n",
      "Od 8442 do 8505 z 11340\n",
      "Od 8505 do 8568 z 11340\n",
      "Od 8568 do 8631 z 11340\n",
      "Od 8631 do 8694 z 11340\n",
      "Od 8694 do 8757 z 11340\n",
      "Od 8757 do 8820 z 11340\n",
      "Od 8820 do 8883 z 11340\n",
      "Od 8883 do 8946 z 11340\n",
      "Od 8946 do 9009 z 11340\n",
      "Od 9009 do 9072 z 11340\n",
      "Od 9072 do 9135 z 11340\n",
      "Od 9135 do 9198 z 11340\n",
      "Od 9198 do 9261 z 11340\n",
      "Od 9261 do 9324 z 11340\n",
      "Od 9324 do 9387 z 11340\n",
      "Od 9387 do 9450 z 11340\n",
      "Od 9450 do 9513 z 11340\n",
      "Od 9513 do 9576 z 11340\n",
      "Od 9576 do 9639 z 11340\n",
      "Od 9639 do 9702 z 11340\n",
      "Od 9702 do 9765 z 11340\n",
      "Od 9765 do 9828 z 11340\n",
      "Od 9828 do 9891 z 11340\n",
      "Od 9891 do 9954 z 11340\n",
      "Od 9954 do 10017 z 11340\n",
      "Od 10017 do 10080 z 11340\n",
      "Od 10080 do 10143 z 11340\n",
      "Od 10143 do 10206 z 11340\n",
      "Od 10206 do 10269 z 11340\n",
      "Od 10269 do 10332 z 11340\n",
      "Od 10332 do 10395 z 11340\n",
      "Od 10395 do 10458 z 11340\n",
      "Od 10458 do 10521 z 11340\n",
      "Od 10521 do 10584 z 11340\n",
      "Od 10584 do 10647 z 11340\n",
      "Od 10647 do 10710 z 11340\n",
      "Od 10710 do 10773 z 11340\n",
      "Od 10773 do 10836 z 11340\n",
      "Od 10836 do 10899 z 11340\n",
      "Od 10899 do 10962 z 11340\n",
      "Od 10962 do 11025 z 11340\n",
      "Od 11025 do 11088 z 11340\n",
      "Od 11088 do 11151 z 11340\n",
      "Od 11151 do 11214 z 11340\n",
      "Od 11214 do 11277 z 11340\n",
      "Od 11277 do 11340 z 11340\n",
      "Przykłady adwersarialne treningowe wygenerowane w czasie: 431.09 s\n",
      "Generowanie 11340 przykładów adwersarialnych...\n",
      "Od 0 do 42 z 1260\n",
      "Od 42 do 84 z 1260\n",
      "Od 84 do 126 z 1260\n",
      "Od 126 do 168 z 1260\n",
      "Od 168 do 210 z 1260\n",
      "Od 210 do 252 z 1260\n",
      "Od 252 do 294 z 1260\n",
      "Od 294 do 336 z 1260\n",
      "Od 336 do 378 z 1260\n",
      "Od 378 do 420 z 1260\n",
      "Od 420 do 462 z 1260\n",
      "Od 462 do 504 z 1260\n",
      "Od 504 do 546 z 1260\n",
      "Od 546 do 588 z 1260\n",
      "Od 588 do 630 z 1260\n",
      "Od 630 do 672 z 1260\n",
      "Od 672 do 714 z 1260\n",
      "Od 714 do 756 z 1260\n",
      "Od 756 do 798 z 1260\n",
      "Od 798 do 840 z 1260\n",
      "Od 840 do 882 z 1260\n",
      "Od 882 do 924 z 1260\n",
      "Od 924 do 966 z 1260\n",
      "Od 966 do 1008 z 1260\n",
      "Od 1008 do 1050 z 1260\n",
      "Od 1050 do 1092 z 1260\n",
      "Od 1092 do 1134 z 1260\n",
      "Od 1134 do 1176 z 1260\n",
      "Od 1176 do 1218 z 1260\n",
      "Od 1218 do 1260 z 1260\n",
      "Łączenie danych oryginalnych i adwersarialnych...\n",
      "x_train_combined shape: (49140, 28, 28, 1)\n",
      "y_train_combined shape: (49140, 10)\n",
      "x_test_combined shape: (5460, 28, 28, 1)\n",
      "y_test_combined shape: (5460, 10)\n",
      "Rozpoczęcie trenowania modelu...\n",
      "Epoch 1/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 36ms/step - accuracy: 0.7817 - loss: 0.7586 - val_accuracy: 0.7773 - val_loss: 3.0264 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 441us/step - accuracy: 0.9375 - loss: 0.1454 - val_accuracy: 0.7773 - val_loss: 3.0429 - learning_rate: 9.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.9577 - loss: 0.1420 - val_accuracy: 0.7839 - val_loss: 2.7652 - learning_rate: 8.1000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step - accuracy: 0.9688 - loss: 0.0879 - val_accuracy: 0.7841 - val_loss: 2.7539 - learning_rate: 7.2900e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 27ms/step - accuracy: 0.9709 - loss: 0.0933 - val_accuracy: 0.7832 - val_loss: 3.1504 - learning_rate: 6.5610e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523us/step - accuracy: 0.9375 - loss: 0.1618 - val_accuracy: 0.7837 - val_loss: 3.1629 - learning_rate: 5.9049e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.9774 - loss: 0.0738 - val_accuracy: 0.7874 - val_loss: 3.2063 - learning_rate: 5.3144e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 848us/step - accuracy: 1.0000 - loss: 0.0280 - val_accuracy: 0.7875 - val_loss: 3.2020 - learning_rate: 4.7830e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 27ms/step - accuracy: 0.9823 - loss: 0.0587 - val_accuracy: 0.7879 - val_loss: 3.0644 - learning_rate: 4.3047e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 531us/step - accuracy: 1.0000 - loss: 0.0054 - val_accuracy: 0.7879 - val_loss: 3.0676 - learning_rate: 3.8742e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.9832 - loss: 0.0550 - val_accuracy: 0.7881 - val_loss: 3.2038 - learning_rate: 3.4868e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 519us/step - accuracy: 0.9615 - loss: 0.0756 - val_accuracy: 0.7881 - val_loss: 3.1947 - learning_rate: 3.1381e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 29ms/step - accuracy: 0.9854 - loss: 0.0484 - val_accuracy: 0.7890 - val_loss: 3.2879 - learning_rate: 2.8243e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522us/step - accuracy: 0.9844 - loss: 0.1424 - val_accuracy: 0.7886 - val_loss: 3.2775 - learning_rate: 2.5419e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.9870 - loss: 0.0419 - val_accuracy: 0.7905 - val_loss: 3.4527 - learning_rate: 2.2877e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - accuracy: 1.0000 - loss: 0.0084 - val_accuracy: 0.7903 - val_loss: 3.4528 - learning_rate: 2.0589e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 28ms/step - accuracy: 0.9888 - loss: 0.0370 - val_accuracy: 0.7883 - val_loss: 3.2369 - learning_rate: 1.8530e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523us/step - accuracy: 0.9844 - loss: 0.0397 - val_accuracy: 0.7881 - val_loss: 3.2390 - learning_rate: 1.6677e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.9896 - loss: 0.0305 - val_accuracy: 0.7896 - val_loss: 3.2542 - learning_rate: 1.5009e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 870us/step - accuracy: 0.9844 - loss: 0.0537 - val_accuracy: 0.7896 - val_loss: 3.2564 - learning_rate: 1.3509e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.9902 - loss: 0.0327 - val_accuracy: 0.7905 - val_loss: 3.5001 - learning_rate: 1.2158e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 851us/step - accuracy: 1.0000 - loss: 0.0053 - val_accuracy: 0.7905 - val_loss: 3.5028 - learning_rate: 1.0942e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 28ms/step - accuracy: 0.9902 - loss: 0.0300 - val_accuracy: 0.7899 - val_loss: 3.4042 - learning_rate: 9.8477e-05\n",
      "Epoch 24/50\n",
      "\u001b[1m767/767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 847us/step - accuracy: 1.0000 - loss: 0.0077 - val_accuracy: 0.7899 - val_loss: 3.4034 - learning_rate: 8.8629e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model wytrenowany dla splitu 30.0% w czasie: 792.85 s\n",
      "\n",
      "Model dla splitu 30.0% zapisany jako AD_model_CW_0.3.h5\n",
      "Czas trwania treningu dla splitu 30.0%: 792.95 sekund\n",
      "\n",
      "Trening modelu dla splitu 40.0% danych adwersarialnych...\n",
      "\n",
      "### Rozpoczynam trenowanie dla splitu 40.0% ###\n",
      "\n",
      "Generowanie 15120 przykładów adwersarialnych...\n",
      "Od 0 do 63 z 15120\n",
      "Od 63 do 126 z 15120\n",
      "Od 126 do 189 z 15120\n",
      "Od 189 do 252 z 15120\n",
      "Od 252 do 315 z 15120\n",
      "Od 315 do 378 z 15120\n",
      "Od 378 do 441 z 15120\n",
      "Od 441 do 504 z 15120\n",
      "Od 504 do 567 z 15120\n",
      "Od 567 do 630 z 15120\n",
      "Od 630 do 693 z 15120\n",
      "Od 693 do 756 z 15120\n",
      "Od 756 do 819 z 15120\n",
      "Od 819 do 882 z 15120\n",
      "Od 882 do 945 z 15120\n",
      "Od 945 do 1008 z 15120\n",
      "Od 1008 do 1071 z 15120\n",
      "Od 1071 do 1134 z 15120\n",
      "Od 1134 do 1197 z 15120\n",
      "Od 1197 do 1260 z 15120\n",
      "Od 1260 do 1323 z 15120\n",
      "Od 1323 do 1386 z 15120\n",
      "Od 1386 do 1449 z 15120\n",
      "Od 1449 do 1512 z 15120\n",
      "Od 1512 do 1575 z 15120\n",
      "Od 1575 do 1638 z 15120\n",
      "Od 1638 do 1701 z 15120\n",
      "Od 1701 do 1764 z 15120\n",
      "Od 1764 do 1827 z 15120\n",
      "Od 1827 do 1890 z 15120\n",
      "Od 1890 do 1953 z 15120\n",
      "Od 1953 do 2016 z 15120\n",
      "Od 2016 do 2079 z 15120\n",
      "Od 2079 do 2142 z 15120\n",
      "Od 2142 do 2205 z 15120\n",
      "Od 2205 do 2268 z 15120\n",
      "Od 2268 do 2331 z 15120\n",
      "Od 2331 do 2394 z 15120\n",
      "Od 2394 do 2457 z 15120\n",
      "Od 2457 do 2520 z 15120\n",
      "Od 2520 do 2583 z 15120\n",
      "Od 2583 do 2646 z 15120\n",
      "Od 2646 do 2709 z 15120\n",
      "Od 2709 do 2772 z 15120\n",
      "Od 2772 do 2835 z 15120\n",
      "Od 2835 do 2898 z 15120\n",
      "Od 2898 do 2961 z 15120\n",
      "Od 2961 do 3024 z 15120\n",
      "Od 3024 do 3087 z 15120\n",
      "Od 3087 do 3150 z 15120\n",
      "Od 3150 do 3213 z 15120\n",
      "Od 3213 do 3276 z 15120\n",
      "Od 3276 do 3339 z 15120\n",
      "Od 3339 do 3402 z 15120\n",
      "Od 3402 do 3465 z 15120\n",
      "Od 3465 do 3528 z 15120\n",
      "Od 3528 do 3591 z 15120\n",
      "Od 3591 do 3654 z 15120\n",
      "Od 3654 do 3717 z 15120\n",
      "Od 3717 do 3780 z 15120\n",
      "Od 3780 do 3843 z 15120\n",
      "Od 3843 do 3906 z 15120\n",
      "Od 3906 do 3969 z 15120\n",
      "Od 3969 do 4032 z 15120\n",
      "Od 4032 do 4095 z 15120\n",
      "Od 4095 do 4158 z 15120\n",
      "Od 4158 do 4221 z 15120\n",
      "Od 4221 do 4284 z 15120\n",
      "Od 4284 do 4347 z 15120\n",
      "Od 4347 do 4410 z 15120\n",
      "Od 4410 do 4473 z 15120\n",
      "Od 4473 do 4536 z 15120\n",
      "Od 4536 do 4599 z 15120\n",
      "Od 4599 do 4662 z 15120\n",
      "Od 4662 do 4725 z 15120\n",
      "Od 4725 do 4788 z 15120\n",
      "Od 4788 do 4851 z 15120\n",
      "Od 4851 do 4914 z 15120\n",
      "Od 4914 do 4977 z 15120\n",
      "Od 4977 do 5040 z 15120\n",
      "Od 5040 do 5103 z 15120\n",
      "Od 5103 do 5166 z 15120\n",
      "Od 5166 do 5229 z 15120\n",
      "Od 5229 do 5292 z 15120\n",
      "Od 5292 do 5355 z 15120\n",
      "Od 5355 do 5418 z 15120\n",
      "Od 5418 do 5481 z 15120\n",
      "Od 5481 do 5544 z 15120\n",
      "Od 5544 do 5607 z 15120\n",
      "Od 5607 do 5670 z 15120\n",
      "Od 5670 do 5733 z 15120\n",
      "Od 5733 do 5796 z 15120\n",
      "Od 5796 do 5859 z 15120\n",
      "Od 5859 do 5922 z 15120\n",
      "Od 5922 do 5985 z 15120\n",
      "Od 5985 do 6048 z 15120\n",
      "Od 6048 do 6111 z 15120\n",
      "Od 6111 do 6174 z 15120\n",
      "Od 6174 do 6237 z 15120\n",
      "Od 6237 do 6300 z 15120\n",
      "Od 6300 do 6363 z 15120\n",
      "Od 6363 do 6426 z 15120\n",
      "Od 6426 do 6489 z 15120\n",
      "Od 6489 do 6552 z 15120\n",
      "Od 6552 do 6615 z 15120\n",
      "Od 6615 do 6678 z 15120\n",
      "Od 6678 do 6741 z 15120\n",
      "Od 6741 do 6804 z 15120\n",
      "Od 6804 do 6867 z 15120\n",
      "Od 6867 do 6930 z 15120\n",
      "Od 6930 do 6993 z 15120\n",
      "Od 6993 do 7056 z 15120\n",
      "Od 7056 do 7119 z 15120\n",
      "Od 7119 do 7182 z 15120\n",
      "Od 7182 do 7245 z 15120\n",
      "Od 7245 do 7308 z 15120\n",
      "Od 7308 do 7371 z 15120\n",
      "Od 7371 do 7434 z 15120\n",
      "Od 7434 do 7497 z 15120\n",
      "Od 7497 do 7560 z 15120\n",
      "Od 7560 do 7623 z 15120\n",
      "Od 7623 do 7686 z 15120\n",
      "Od 7686 do 7749 z 15120\n",
      "Od 7749 do 7812 z 15120\n",
      "Od 7812 do 7875 z 15120\n",
      "Od 7875 do 7938 z 15120\n",
      "Od 7938 do 8001 z 15120\n",
      "Od 8001 do 8064 z 15120\n",
      "Od 8064 do 8127 z 15120\n",
      "Od 8127 do 8190 z 15120\n",
      "Od 8190 do 8253 z 15120\n",
      "Od 8253 do 8316 z 15120\n",
      "Od 8316 do 8379 z 15120\n",
      "Od 8379 do 8442 z 15120\n",
      "Od 8442 do 8505 z 15120\n",
      "Od 8505 do 8568 z 15120\n",
      "Od 8568 do 8631 z 15120\n",
      "Od 8631 do 8694 z 15120\n",
      "Od 8694 do 8757 z 15120\n",
      "Od 8757 do 8820 z 15120\n",
      "Od 8820 do 8883 z 15120\n",
      "Od 8883 do 8946 z 15120\n",
      "Od 8946 do 9009 z 15120\n",
      "Od 9009 do 9072 z 15120\n",
      "Od 9072 do 9135 z 15120\n",
      "Od 9135 do 9198 z 15120\n",
      "Od 9198 do 9261 z 15120\n",
      "Od 9261 do 9324 z 15120\n",
      "Od 9324 do 9387 z 15120\n",
      "Od 9387 do 9450 z 15120\n",
      "Od 9450 do 9513 z 15120\n",
      "Od 9513 do 9576 z 15120\n",
      "Od 9576 do 9639 z 15120\n",
      "Od 9639 do 9702 z 15120\n",
      "Od 9702 do 9765 z 15120\n",
      "Od 9765 do 9828 z 15120\n",
      "Od 9828 do 9891 z 15120\n",
      "Od 9891 do 9954 z 15120\n",
      "Od 9954 do 10017 z 15120\n",
      "Od 10017 do 10080 z 15120\n",
      "Od 10080 do 10143 z 15120\n",
      "Od 10143 do 10206 z 15120\n",
      "Od 10206 do 10269 z 15120\n",
      "Od 10269 do 10332 z 15120\n",
      "Od 10332 do 10395 z 15120\n",
      "Od 10395 do 10458 z 15120\n",
      "Od 10458 do 10521 z 15120\n",
      "Od 10521 do 10584 z 15120\n",
      "Od 10584 do 10647 z 15120\n",
      "Od 10647 do 10710 z 15120\n",
      "Od 10710 do 10773 z 15120\n",
      "Od 10773 do 10836 z 15120\n",
      "Od 10836 do 10899 z 15120\n",
      "Od 10899 do 10962 z 15120\n",
      "Od 10962 do 11025 z 15120\n",
      "Od 11025 do 11088 z 15120\n",
      "Od 11088 do 11151 z 15120\n",
      "Od 11151 do 11214 z 15120\n",
      "Od 11214 do 11277 z 15120\n",
      "Od 11277 do 11340 z 15120\n",
      "Od 11340 do 11403 z 15120\n",
      "Od 11403 do 11466 z 15120\n",
      "Od 11466 do 11529 z 15120\n",
      "Od 11529 do 11592 z 15120\n",
      "Od 11592 do 11655 z 15120\n",
      "Od 11655 do 11718 z 15120\n",
      "Od 11718 do 11781 z 15120\n",
      "Od 11781 do 11844 z 15120\n",
      "Od 11844 do 11907 z 15120\n",
      "Od 11907 do 11970 z 15120\n",
      "Od 11970 do 12033 z 15120\n",
      "Od 12033 do 12096 z 15120\n",
      "Od 12096 do 12159 z 15120\n",
      "Od 12159 do 12222 z 15120\n",
      "Od 12222 do 12285 z 15120\n",
      "Od 12285 do 12348 z 15120\n",
      "Od 12348 do 12411 z 15120\n",
      "Od 12411 do 12474 z 15120\n",
      "Od 12474 do 12537 z 15120\n",
      "Od 12537 do 12600 z 15120\n",
      "Od 12600 do 12663 z 15120\n",
      "Od 12663 do 12726 z 15120\n",
      "Od 12726 do 12789 z 15120\n",
      "Od 12789 do 12852 z 15120\n",
      "Od 12852 do 12915 z 15120\n",
      "Od 12915 do 12978 z 15120\n",
      "Od 12978 do 13041 z 15120\n",
      "Od 13041 do 13104 z 15120\n",
      "Od 13104 do 13167 z 15120\n",
      "Od 13167 do 13230 z 15120\n",
      "Od 13230 do 13293 z 15120\n",
      "Od 13293 do 13356 z 15120\n",
      "Od 13356 do 13419 z 15120\n",
      "Od 13419 do 13482 z 15120\n",
      "Od 13482 do 13545 z 15120\n",
      "Od 13545 do 13608 z 15120\n",
      "Od 13608 do 13671 z 15120\n",
      "Od 13671 do 13734 z 15120\n",
      "Od 13734 do 13797 z 15120\n",
      "Od 13797 do 13860 z 15120\n",
      "Od 13860 do 13923 z 15120\n",
      "Od 13923 do 13986 z 15120\n",
      "Od 13986 do 14049 z 15120\n",
      "Od 14049 do 14112 z 15120\n",
      "Od 14112 do 14175 z 15120\n",
      "Od 14175 do 14238 z 15120\n",
      "Od 14238 do 14301 z 15120\n",
      "Od 14301 do 14364 z 15120\n",
      "Od 14364 do 14427 z 15120\n",
      "Od 14427 do 14490 z 15120\n",
      "Od 14490 do 14553 z 15120\n",
      "Od 14553 do 14616 z 15120\n",
      "Od 14616 do 14679 z 15120\n",
      "Od 14679 do 14742 z 15120\n",
      "Od 14742 do 14805 z 15120\n",
      "Od 14805 do 14868 z 15120\n",
      "Od 14868 do 14931 z 15120\n",
      "Od 14931 do 14994 z 15120\n",
      "Od 14994 do 15057 z 15120\n",
      "Od 15057 do 15120 z 15120\n",
      "Przykłady adwersarialne treningowe wygenerowane w czasie: 568.07 s\n",
      "Generowanie 15120 przykładów adwersarialnych...\n",
      "Od 0 do 42 z 1680\n",
      "Od 42 do 84 z 1680\n",
      "Od 84 do 126 z 1680\n",
      "Od 126 do 168 z 1680\n",
      "Od 168 do 210 z 1680\n",
      "Od 210 do 252 z 1680\n",
      "Od 252 do 294 z 1680\n",
      "Od 294 do 336 z 1680\n",
      "Od 336 do 378 z 1680\n",
      "Od 378 do 420 z 1680\n",
      "Od 420 do 462 z 1680\n",
      "Od 462 do 504 z 1680\n",
      "Od 504 do 546 z 1680\n",
      "Od 546 do 588 z 1680\n",
      "Od 588 do 630 z 1680\n",
      "Od 630 do 672 z 1680\n",
      "Od 672 do 714 z 1680\n",
      "Od 714 do 756 z 1680\n",
      "Od 756 do 798 z 1680\n",
      "Od 798 do 840 z 1680\n",
      "Od 840 do 882 z 1680\n",
      "Od 882 do 924 z 1680\n",
      "Od 924 do 966 z 1680\n",
      "Od 966 do 1008 z 1680\n",
      "Od 1008 do 1050 z 1680\n",
      "Od 1050 do 1092 z 1680\n",
      "Od 1092 do 1134 z 1680\n",
      "Od 1134 do 1176 z 1680\n",
      "Od 1176 do 1218 z 1680\n",
      "Od 1218 do 1260 z 1680\n",
      "Od 1260 do 1302 z 1680\n",
      "Od 1302 do 1344 z 1680\n",
      "Od 1344 do 1386 z 1680\n",
      "Od 1386 do 1428 z 1680\n",
      "Od 1428 do 1470 z 1680\n",
      "Od 1470 do 1512 z 1680\n",
      "Od 1512 do 1554 z 1680\n",
      "Od 1554 do 1596 z 1680\n",
      "Od 1596 do 1638 z 1680\n",
      "Od 1638 do 1680 z 1680\n",
      "Łączenie danych oryginalnych i adwersarialnych...\n",
      "x_train_combined shape: (52920, 28, 28, 1)\n",
      "y_train_combined shape: (52920, 10)\n",
      "x_test_combined shape: (5880, 28, 28, 1)\n",
      "y_test_combined shape: (5880, 10)\n",
      "Rozpoczęcie trenowania modelu...\n",
      "Epoch 1/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 36ms/step - accuracy: 0.7859 - loss: 0.7539 - val_accuracy: 0.7262 - val_loss: 3.5103 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 820us/step - accuracy: 0.9531 - loss: 0.1281 - val_accuracy: 0.7262 - val_loss: 3.5177 - learning_rate: 9.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 27ms/step - accuracy: 0.9580 - loss: 0.1375 - val_accuracy: 0.7372 - val_loss: 3.8138 - learning_rate: 8.1000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 785us/step - accuracy: 0.9375 - loss: 0.1065 - val_accuracy: 0.7367 - val_loss: 3.8225 - learning_rate: 7.2900e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 27ms/step - accuracy: 0.9731 - loss: 0.0859 - val_accuracy: 0.7372 - val_loss: 3.5623 - learning_rate: 6.5610e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 787us/step - accuracy: 0.9844 - loss: 0.0342 - val_accuracy: 0.7372 - val_loss: 3.5536 - learning_rate: 5.9049e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 28ms/step - accuracy: 0.9772 - loss: 0.0747 - val_accuracy: 0.7374 - val_loss: 3.9421 - learning_rate: 5.3144e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527us/step - accuracy: 0.9688 - loss: 0.0562 - val_accuracy: 0.7366 - val_loss: 3.9965 - learning_rate: 4.7830e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 27ms/step - accuracy: 0.9815 - loss: 0.0593 - val_accuracy: 0.7367 - val_loss: 4.0309 - learning_rate: 4.3047e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 804us/step - accuracy: 0.9531 - loss: 0.0916 - val_accuracy: 0.7367 - val_loss: 4.0397 - learning_rate: 3.8742e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 27ms/step - accuracy: 0.9828 - loss: 0.0563 - val_accuracy: 0.7395 - val_loss: 3.9989 - learning_rate: 3.4868e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 785us/step - accuracy: 0.9844 - loss: 0.0551 - val_accuracy: 0.7395 - val_loss: 3.9915 - learning_rate: 3.1381e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 28ms/step - accuracy: 0.9852 - loss: 0.0457 - val_accuracy: 0.7406 - val_loss: 4.2347 - learning_rate: 2.8243e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 505us/step - accuracy: 1.0000 - loss: 0.0081 - val_accuracy: 0.7406 - val_loss: 4.2461 - learning_rate: 2.5419e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 27ms/step - accuracy: 0.9868 - loss: 0.0428 - val_accuracy: 0.7401 - val_loss: 4.0909 - learning_rate: 2.2877e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - accuracy: 0.9844 - loss: 0.0193 - val_accuracy: 0.7401 - val_loss: 4.0946 - learning_rate: 2.0589e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 28ms/step - accuracy: 0.9874 - loss: 0.0401 - val_accuracy: 0.7408 - val_loss: 4.1780 - learning_rate: 1.8530e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518us/step - accuracy: 1.0000 - loss: 0.0099 - val_accuracy: 0.7408 - val_loss: 4.1785 - learning_rate: 1.6677e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 28ms/step - accuracy: 0.9892 - loss: 0.0338 - val_accuracy: 0.7412 - val_loss: 4.2150 - learning_rate: 1.5009e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 505us/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.7412 - val_loss: 4.2113 - learning_rate: 1.3509e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 26ms/step - accuracy: 0.9908 - loss: 0.0315 - val_accuracy: 0.7410 - val_loss: 4.3074 - learning_rate: 1.2158e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model wytrenowany dla splitu 40.0% w czasie: 950.77 s\n",
      "\n",
      "Model dla splitu 40.0% zapisany jako AD_model_CW_0.4.h5\n",
      "Czas trwania treningu dla splitu 40.0%: 950.90 sekund\n",
      "\n",
      "Wszystkie modele zostały wytrenowane i zapisane.\n"
     ]
    }
   ],
   "source": [
    "# Train and Save Models\n",
    "for split in splits:\n",
    "    start_time = time.time()\n",
    "    print(f\"\\nTrening modelu dla splitu {split * 100}% danych adwersarialnych...\")\n",
    "\n",
    "    # Trenowanie modelu\n",
    "    model = adversarial_training(x_train, y_train, model, x_test, y_test, batch_size, epochs, split=split, attack_type = \"CW\", c = 0.3)\n",
    "\n",
    "    # Nazwa pliku\n",
    "    model_filename = f\"AD_model_CW_{split:.1f}.h5\"\n",
    "\n",
    "    # Zapis modelu w bieżącym katalogu\n",
    "    model.save(model_filename)\n",
    "    print(f\"Model dla splitu {split * 100}% zapisany jako {model_filename}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    iteration_time = end_time - start_time\n",
    "    print(f\"Czas trwania treningu dla splitu {split * 100}%: {iteration_time:.2f} sekund\")\n",
    "\n",
    "print(\"\\nWszystkie modele zostały wytrenowane i zapisane.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMShwgao7b4rF6yPo+TWEgu",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
